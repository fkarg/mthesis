@book{agarwal_introduction_2008,
  title = {An {{Introduction}} to {{Ordinary Differential Equations}}},
  author = {Agarwal, Ravi P. and O'Regan, Donal},
  year = {2008},
  publisher = {{Springer New York}},
  address = {{New York, NY}},
  doi = {10.1007/978-0-387-71276-5},
  url = {http://link.springer.com/10.1007/978-0-387-71276-5},
  urldate = {2023-01-18},
  isbn = {978-0-387-71275-8 978-0-387-71276-5},
  language = {en}
}

@article{alayrac_flamingo_2022,
  title = {Flamingo: A {{Visual Language Model}} for {{Few-Shot Learning}}},
  shorttitle = {Flamingo},
  author = {Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katie and Reynolds, Malcolm and Ring, Roman and Rutherford, Eliza and Cabi, Serkan and Han, Tengda and Gong, Zhitao and Samangooei, Sina and Monteiro, Marianne and Menick, Jacob and Borgeaud, Sebastian and Brock, Andrew and Nematzadeh, Aida and Sharifzadeh, Sahand and Binkowski, Mikolaj and Barreira, Ricardo and Vinyals, Oriol and Zisserman, Andrew and Simonyan, Karen},
  year = {2022},
  month = nov,
  number = {arXiv:2204.14198},
  eprint = {2204.14198},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2204.14198},
  url = {http://arxiv.org/abs/2204.14198},
  urldate = {2023-01-18},
  abstract = {Building models that can be rapidly adapted to novel tasks using only a handful of annotated examples is an open challenge for multimodal machine learning research. We introduce Flamingo, a family of Visual Language Models (VLM) with this ability. We propose key architectural innovations to: (i) bridge powerful pretrained vision-only and language-only models, (ii) handle sequences of arbitrarily interleaved visual and textual data, and (iii) seamlessly ingest images or videos as inputs. Thanks to their flexibility, Flamingo models can be trained on large-scale multimodal web corpora containing arbitrarily interleaved text and images, which is key to endow them with in-context few-shot learning capabilities. We perform a thorough evaluation of our models, exploring and measuring their ability to rapidly adapt to a variety of image and video tasks. These include open-ended tasks such as visual question-answering, where the model is prompted with a question which it has to answer; captioning tasks, which evaluate the ability to describe a scene or an event; and close-ended tasks such as multiple-choice visual question-answering. For tasks lying anywhere on this spectrum, a single Flamingo model can achieve a new state of the art with few-shot learning, simply by prompting the model with task-specific examples. On numerous benchmarks, Flamingo outperforms models fine-tuned on thousands of times more task-specific data.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,DeepMind},
  journal = {arXiv:2204.14198}
}

@article{allauzen_experimental_2022,
  title = {Experimental Study of {{Neural ODE}} Training with Adaptive Solver for Dynamical Systems Modeling},
  author = {Allauzen, Alexandre and Dardis, Thiago Petrilli Maffei and Plath, Hannah},
  year = {2022},
  month = nov,
  number = {arXiv:2211.06972},
  eprint = {2211.06972},
  eprinttype = {arxiv},
  primaryclass = {nlin},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2211.06972},
  url = {http://arxiv.org/abs/2211.06972},
  urldate = {2023-01-18},
  abstract = {Neural Ordinary Differential Equations (ODEs) was recently introduced as a new family of neural network models, which relies on black-box ODE solvers for inference and training. Some ODE solvers called adaptive can adapt their evaluation strategy depending on the complexity of the problem at hand, opening great perspectives in machine learning. However, this paper describes a simple set of experiments to show why adaptive solvers cannot be seamlessly leveraged as a black-box for dynamical systems modelling. By taking the Lorenz'63 system as a showcase, we show that a naive application of the Fehlberg's method does not yield the expected results. Moreover, a simple workaround is proposed that assumes a tighter interaction between the solver and the training strategy. The code is available on github: https://github.com/Allauzen/adaptive-step-size-neural-ode},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,G.1,Nonlinear Sciences - Chaotic Dynamics},
  journal = {arXiv:2211.06972}
}

@article{baburoglu_novel_2021,
  title = {Novel Hybrid Pair Recommendations Based on a Large-Scale Comparative Study of Concept Drift Detection},
  author = {Babüroğlu, Elif Selen and Durmuşoğlu, Alptekin and Dereli, Türkay},
  year = {2021},
  journal = {Expert Systems with Applications},
  volume = {163},
  pages = {113786},
  publisher = {{Elsevier}},
  doi = {10.1016/j.eswa.2020.113786}
}

@article{barros_largescale_2018,
  title = {A Large-Scale Comparison of Concept Drift Detectors},
  author = {Barros, Roberto Souto Maior and Santos, Silas Garrido T. Carvalho},
  year = {2018},
  journal = {Information Sciences},
  volume = {451},
  pages = {348--370},
  publisher = {{Elsevier}},
  doi = {10.1016/j.ins.2018.04.014}
}

@article{beard_comparative_2019,
  title = {Comparative Dataset of Experimental and Computational Attributes of {{UV}}/Vis Absorption Spectra},
  author = {Beard, Edward J. and Sivaraman, Ganesh and {Vazquez-Mayagoitia}, Alvaro and Vishwanath, Venkatram and Cole, Jacqueline M.},
  year = {2019},
  month = dec,
  journal = {Scientific Data},
  volume = {6},
  number = {1},
  pages = {307},
  publisher = {{Nature Publishing Group}},
  issn = {2052-4463},
  doi = {10.1038/s41597-019-0306-0},
  url = {https://www.nature.com/articles/s41597-019-0306-0},
  urldate = {2023-02-20},
  abstract = {The ability to auto-generate databases of optical properties holds great prospects in data-driven materials discovery for optoelectronic applications. We present a cognate set of experimental and computational data that describes key features of optical absorption spectra. This includes an auto-generated database of 18,309 records of experimentally determined UV/vis absorption maxima, λmax, and associated extinction coefficients, ϵ, where present. This database was produced using the text-mining toolkit, ChemDataExtractor, on 402,034 scientific documents. High-throughput electronic-structure calculations using fast (simplified Tamm-Dancoff approach) and traditional (time-dependent) density functional theory were executed to predict λmax and oscillation strengths, f (related to ϵ) for a subset of validated compounds. Paired quantities of these computational and experimental data show strong correlations in λmax, f and ϵ, laying the path for reliable in silico calculations of additional optical properties. The total dataset of 8,488 unique compounds and a subset of 5,380 compounds with experimental and computational data, are available in MongoDB, CSV and JSON formats. These can be queried using Python, R, Java, and MATLAB, for data-driven optoelectronic materials discovery.},
  copyright = {2019 The Author(s)},
  language = {en},
  keywords = {Cheminformatics,Computational methods,Materials for optics,Optical materials}
}

@article{bilos_neural_2021,
  title = {Neural {{Flows}}: {{Efficient Alternative}} to {{Neural ODEs}}},
  shorttitle = {Neural {{Flows}}},
  author = {Biloš, Marin and Sommer, Johanna and Rangapuram, Syama Sundar and Januschowski, Tim and Günnemann, Stephan},
  year = {2021},
  journal = {Advances in Neural Information Processing Systems},
  volume = {34},
  pages = {21325--21337},
  keywords = {⛔ No DOI found}
}

@inproceedings{bogdoll_anomaly_2022,
  title = {Anomaly {{Detection}} in {{Autonomous Driving}}: {{A Survey}}},
  shorttitle = {Anomaly {{Detection}} in {{Autonomous Driving}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Bogdoll, Daniel and Nitsche, Maximilian and Zöllner, J. Marius},
  year = {2022},
  pages = {4488--4499},
  keywords = {⛔ No DOI found}
}

@article{borji_categorical_2023,
  title = {A {{Categorical Archive}} of {{ChatGPT Failures}}},
  author = {Borji, Ali},
  year = {2023},
  month = feb,
  number = {arXiv:2302.03494},
  eprint = {2302.03494},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2302.03494},
  url = {http://arxiv.org/abs/2302.03494},
  urldate = {2023-02-08},
  abstract = {Large language models have been demonstrated to be valuable in different fields. ChatGPT, developed by OpenAI, has been trained using massive amounts of data and simulates human conversation by comprehending context and generating appropriate responses. It has garnered significant attention due to its ability to effectively answer a broad range of human inquiries, with fluent and comprehensive answers surpassing prior public chatbots in both security and usefulness. However, a comprehensive analysis of ChatGPT's failures is lacking, which is the focus of this study. Ten categories of failures, including reasoning, factual errors, math, coding, and bias, are presented and discussed. The risks, limitations, and societal implications of ChatGPT are also highlighted. The goal of this study is to assist researchers and developers in enhancing future language models and chatbots.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  journal = {arXiv:2302.03494}
}

@article{breitenstein_corner_2021,
  title = {Corner Cases for Visual Perception in Automated Driving: {{Some}} Guidance on Detection Approaches},
  shorttitle = {Corner Cases for Visual Perception in Automated Driving},
  author = {Breitenstein, Jasmin and Termöhlen, Jan-Aike and Lipinski, Daniel and Fingscheidt, Tim},
  year = {2021},
  journal = {arXiv preprint arXiv:2102.05897},
  eprint = {2102.05897},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  keywords = {⛔ No DOI found}
}

@inproceedings{breitenstein_systematization_2020,
  title = {Systematization of {{Corner Cases}} for {{Visual Perception}} in {{Automated Driving}}},
  booktitle = {2020 {{IEEE Intelligent Vehicles Symposium}} ({{IV}})},
  author = {Breitenstein, Jasmin and Termöhlen, Jan-Aike and Lipinski, Daniel and Fingscheidt, Tim},
  year = {2020},
  month = oct,
  pages = {1257--1264},
  issn = {2642-7214},
  doi = {10.1109/IV47402.2020.9304789},
  abstract = {One major task in automated driving is the development of robust and safe visual perception modules. It is of utmost importance that visual perception reacts adequately to so-called corner cases, which range from overexposure of the image sensor to unexpected and potentially dangerous traffic situations. Their detection thus has high significance both as an online system in the intelligent vehicle, but also in the extraction of relevant training and test data for perception modules. In this paper, we provide a systematization of corner cases for visual perception in automated driving, with the categories being structured by detection complexity. Furthermore, we discuss existing metrics and datasets which can be used for the evaluation of corner case detection methods depending on their suitability to provide beneficial information for the various categories.},
  keywords = {Automobiles,Complexity theory,Roads,Semantics,Training,Visual perception,Visualization}
}

@article{brown_language_2020,
  title = {Language Models Are Few-Shot Learners},
  author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D. and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda},
  year = {2020},
  journal = {Advances in neural information processing systems},
  volume = {33},
  pages = {1877--1901},
  keywords = {⛔ No DOI found}
}

@inproceedings{chang_reversible_2018,
  title = {Reversible Architectures for Arbitrarily Deep Residual Neural Networks},
  booktitle = {Proceedings of the {{AAAI}} Conference on Artificial Intelligence},
  author = {Chang, Bo and Meng, Lili and Haber, Eldad and Ruthotto, Lars and Begert, David and Holtham, Elliot},
  year = {2018},
  volume = {32},
  doi = {10.1609/aaai.v32i1.11668}
}

@article{chen_neural_2019,
  title = {Neural {{Ordinary Differential Equations}}},
  author = {Chen, Ricky T. Q. and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David},
  year = {2019},
  month = dec,
  number = {arXiv:1806.07366},
  eprint = {1806.07366},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1806.07366},
  urldate = {2023-01-18},
  abstract = {We introduce a new family of deep neural network models. Instead of specifying a discrete sequence of hidden layers, we parameterize the derivative of the hidden state using a neural network. The output of the network is computed using a blackbox differential equation solver. These continuous-depth models have constant memory cost, adapt their evaluation strategy to each input, and can explicitly trade numerical precision for speed. We demonstrate these properties in continuous-depth residual networks and continuous-time latent variable models. We also construct continuous normalizing flows, a generative model that can train by maximum likelihood, without partitioning or ordering the data dimensions. For training, we show how to scalably backpropagate through any ODE solver, without access to its internal operations. This allows end-to-end training of ODEs within larger models.},
  archiveprefix = {arXiv},
  language = {en},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  journal = {arXiv:1806.07366}
}

@article{chen_residual_2019,
  title = {Residual Flows for Invertible Generative Modeling},
  author = {Chen, Ricky TQ and Behrmann, Jens and Duvenaud, David K. and Jacobsen, Jörn-Henrik},
  year = {2019},
  journal = {Advances in Neural Information Processing Systems},
  volume = {32},
  keywords = {⛔ No DOI found}
}

@article{choromanski_ode_2020,
  title = {Ode to an {{ODE}}},
  author = {Choromanski, Krzysztof M. and Davis, Jared Quincy and Likhosherstov, Valerii and Song, Xingyou and Slotine, Jean-Jacques and Varley, Jacob and Lee, Honglak and Weller, Adrian and Sindhwani, Vikas},
  year = {2020},
  journal = {Advances in Neural Information Processing Systems},
  volume = {33},
  pages = {3338--3350},
  keywords = {⛔ No DOI found}
}

@article{creswell_generative_2018,
  title = {Generative {{Adversarial Networks}}: {{An Overview}}},
  shorttitle = {Generative {{Adversarial Networks}}},
  author = {Creswell, Antonia and White, Tom and Dumoulin, Vincent and Arulkumaran, Kai and Sengupta, Biswa and Bharath, Anil A.},
  year = {2018},
  month = jan,
  journal = {IEEE Signal Processing Magazine},
  volume = {35},
  number = {1},
  pages = {53--65},
  issn = {1558-0792},
  doi = {10.1109/MSP.2017.2765202},
  abstract = {Generative adversarial networks (GANs) provide a way to learn deep representations without extensively annotated training data. They achieve this by deriving backpropagation signals through a competitive process involving a pair of networks. The representations that can be learned by GANs may be used in a variety of applications, including image synthesis, semantic image editing, style transfer, image superresolution, and classification. The aim of this review article is to provide an overview of GANs for the signal processing community, drawing on familiar analogies and concepts where possible. In addition to identifying different methods for training and constructing GANs, we also point to remaining challenges in their theory and application.},
  keywords = {Convolutional codes,Data models,Generators,Image resolution,Machine learning,Semantics,Signal resolution,Training data}
}

@article{creswell_selectioninference_2022,
  title = {Selection-{{Inference}}: {{Exploiting Large Language Models}} for {{Interpretable Logical Reasoning}}},
  shorttitle = {Selection-{{Inference}}},
  author = {Creswell, Antonia and Shanahan, Murray and Higgins, Irina},
  year = {2022},
  month = may,
  number = {arXiv:2205.09712},
  eprint = {2205.09712},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2205.09712},
  url = {http://arxiv.org/abs/2205.09712},
  urldate = {2023-02-09},
  abstract = {Large language models (LLMs) have been shown to be capable of impressive few-shot generalisation to new tasks. However, they still tend to perform poorly on multi-step logical reasoning problems. Here we carry out a comprehensive evaluation of LLMs on 50 tasks that probe different aspects of logical reasoning. We show that language models tend to perform fairly well at single step inference or entailment tasks, but struggle to chain together multiple reasoning steps to solve more complex problems. In light of this, we propose a Selection-Inference (SI) framework that exploits pre-trained LLMs as general processing modules, and alternates between selection and inference to generate a series of interpretable, casual reasoning steps leading to the final answer. We show that a 7B parameter LLM used within the SI framework in a 5-shot generalisation setting, with no fine-tuning, yields a performance improvement of over 100\% compared to an equivalent vanilla baseline on a suite of 10 logical reasoning tasks. The same model in the same setting even outperforms a significantly larger 280B parameter baseline on the same suite of tasks. Moreover, answers produced by the SI framework are accompanied by a causal natural-language-based reasoning trace, which has important implications for the safety and trustworthiness of the system.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  journal = {arXiv:2205.09712}
}

@article{devlin_bert_2018,
  title = {Bert: {{Pre-training}} of Deep Bidirectional Transformers for Language Understanding},
  shorttitle = {Bert},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  year = {2018},
  journal = {arXiv preprint arXiv:1810.04805},
  eprint = {1810.04805},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  keywords = {⛔ No DOI found}
}

@inproceedings{dong_adaptive_2020,
  title = {Towards {{Adaptive Residual Network Training}}: {{A Neural-ODE Perspective}}},
  shorttitle = {Towards {{Adaptive Residual Network Training}}},
  booktitle = {Proceedings of the 37th {{International Conference}} on {{Machine Learning}}},
  author = {Dong, Chengyu and Liu, Liyuan and Li, Zichao and Shang, Jingbo},
  year = {2020},
  month = nov,
  pages = {2616--2626},
  publisher = {{PMLR}},
  url = {https://proceedings.mlr.press/v119/dong20c.html},
  urldate = {2023-02-04},
  abstract = {In pursuit of resource-economical machine learning, attempts have been made to dynamically adjust computation workloads in different training stages, i.e., starting with a shallow network and gradually increasing the model depth (and computation workloads) during training. However, there is neither guarantee nor guidance on designing such network grow, due to the lack of its theoretical underpinnings. In this work, to explore the theory behind, we conduct theoretical analyses from an ordinary differential equation perspective. Specifically, we illustrate the dynamics of network growth and propose a novel performance measure specific to the depth increase. Illuminated by our analyses, we move towards theoretically sound growing operations and schedulers, giving rise to an adaptive training algorithm for residual networks, LipGrow, which automatically increases network depth thus accelerates training. In our experiments, it achieves comparable performance while reducing ∼ 50\% of training time.},
  language = {en}
}

@article{du_vos_2022,
  title = {Vos: {{Learning}} What You Don't Know by Virtual Outlier Synthesis},
  shorttitle = {Vos},
  author = {Du, Xuefeng and Wang, Zhaoning and Cai, Mu and Li, Yixuan},
  year = {2022},
  journal = {arXiv preprint arXiv:2202.01197},
  eprint = {2202.01197},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  keywords = {⛔ No DOI found}
}

@article{dunn_structured_2022,
  title = {Structured Information Extraction from Complex Scientific Text with Fine-Tuned Large Language Models},
  author = {Dunn, Alexander and Dagdelen, John and Walker, Nicholas and Lee, Sanghoon and Rosen, Andrew S. and Ceder, Gerbrand and Persson, Kristin and Jain, Anubhav},
  year = {2022},
  month = dec,
  number = {arXiv:2212.05238},
  eprint = {2212.05238},
  eprinttype = {arxiv},
  primaryclass = {cond-mat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2212.05238},
  url = {http://arxiv.org/abs/2212.05238},
  urldate = {2023-02-01},
  abstract = {Intelligently extracting and linking complex scientific information from unstructured text is a challenging endeavor particularly for those inexperienced with natural language processing. Here, we present a simple sequence-to-sequence approach to joint named entity recognition and relation extraction for complex hierarchical information in scientific text. The approach leverages a pre-trained large language model (LLM), GPT-3, that is fine-tuned on approximately 500 pairs of prompts (inputs) and completions (outputs). Information is extracted either from single sentences or across sentences in abstracts/passages, and the output can be returned as simple English sentences or a more structured format, such as a list of JSON objects. We demonstrate that LLMs trained in this way are capable of accurately extracting useful records of complex scientific knowledge for three representative tasks in materials chemistry: linking dopants with their host materials, cataloging metal-organic frameworks, and general chemistry/phase/morphology/application information extraction. This approach represents a simple, accessible, and highly-flexible route to obtaining large databases of structured knowledge extracted from unstructured text. An online demo is available at http://www.matscholar.com/info-extraction.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Condensed Matter - Materials Science,I.7.m},
  journal = {arXiv:2212.05238}
}

@article{dupont_augmented_2019,
  title = {Augmented Neural Odes},
  author = {Dupont, Emilien and Doucet, Arnaud and Teh, Yee Whye},
  year = {2019},
  journal = {Advances in Neural Information Processing Systems},
  volume = {32},
  keywords = {⛔ No DOI found}
}

@article{eliasof_pdegcn_2021,
  title = {Pde-Gcn: {{Novel}} Architectures for Graph Neural Networks Motivated by Partial Differential Equations},
  shorttitle = {Pde-Gcn},
  author = {Eliasof, Moshe and Haber, Eldad and Treister, Eran},
  year = {2021},
  journal = {Advances in Neural Information Processing Systems},
  volume = {34},
  pages = {3836--3849},
  keywords = {⛔ No DOI found}
}

@article{finlay_how_2020,
  title = {How to Train Your Neural Ode},
  author = {Finlay, Chris and Jacobsen, Jörn-Henrik and Nurbekyan, Levon and Oberman, Adam M.},
  year = {2020},
  journal = {arXiv preprint arXiv:2002.02798},
  eprint = {2002.02798},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  keywords = {⛔ No DOI found}
}

@article{girija_tensorflow_2016,
  title = {Tensorflow: {{Large-scale}} Machine Learning on Heterogeneous Distributed Systems},
  shorttitle = {Tensorflow},
  author = {Girija, Sanjay Surendranath},
  year = {2016},
  journal = {Software available from tensorflow. org},
  volume = {39},
  number = {9},
  keywords = {⛔ No DOI found}
}

@inproceedings{girshick_fast_2015,
  title = {Fast R-Cnn},
  booktitle = {Proceedings of the {{IEEE}} International Conference on Computer Vision},
  author = {Girshick, Ross},
  year = {2015},
  pages = {1440--1448},
  keywords = {⛔ No DOI found}
}

@article{goodfellow_generative_2020,
  title = {Generative Adversarial Networks},
  author = {Goodfellow, Ian and {Pouget-Abadie}, Jean and Mirza, Mehdi and Xu, Bing and {Warde-Farley}, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  year = {2020},
  journal = {Communications of the ACM},
  volume = {63},
  number = {11},
  pages = {139--144},
  publisher = {{ACM New York, NY, USA}},
  doi = {10.1145/3422622},
  keywords = {GAN}
}

@article{gordon_novel_1993,
  title = {Novel Approach to Nonlinear/Non-{{Gaussian Bayesian}} State Estimation},
  author = {Gordon, N.J. and Salmond, D.J. and Smith, A.F.M.},
  year = {1993},
  journal = {IEE Proceedings F Radar and Signal Processing},
  volume = {140},
  number = {2},
  pages = {107},
  issn = {0956375X},
  doi = {10.1049/ip-f-2.1993.0015},
  url = {https://digital-library.theiet.org/content/journals/10.1049/ip-f-2.1993.0015},
  urldate = {2023-01-19},
  abstract = {An algorithm, the bootstrap filter, is proposed for implementing recursive Bayesian filters. The required density of the state vector is represented as a set of random samples, which are updated and propagated by the algorithm. The method is not restricted by assumptions of linearity or Gaussian noise: it may be applied to any state transition or measurement model. A simulation example of the bearings only tracking problem is presented. This simulation includes schemes for improving the efficiency of the basic algorithm. For this example, the performance of the bootstrap filter is greatly superior to the standard extended Kalman filter.},
  language = {en}
}

@article{gui_review_2021,
  title = {A {{Review}} on {{Generative Adversarial Networks}}: {{Algorithms}}, {{Theory}}, and {{Applications}}},
  shorttitle = {A {{Review}} on {{Generative Adversarial Networks}}},
  author = {Gui, Jie and Sun, Zhenan and Wen, Yonggang and Tao, Dacheng and Ye, Jieping},
  year = {2021},
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  pages = {1--1},
  issn = {1558-2191},
  doi = {10.1109/TKDE.2021.3130191},
  abstract = {Generative adversarial networks (GANs) have recently become a hot research topic; however, they have been studied since 2014, and a large number of algorithms have been proposed. However, few comprehensive studies exist explaining the connections among different GANs variants and how they have evolved. In this paper, we attempt to provide a review of the various GANs methods from the perspectives of algorithms, theory, and applications. First, the motivations, mathematical representations, and structures of most GANs algorithms are introduced in detail and we compare their commonalities and differences. Second, theoretical issues related to GANs are investigated. Finally, typical applications of GANs in image processing and computer vision, natural language processing, music, speech and audio, medical field, and data science are discussed.},
  keywords = {Algorithm,Applications,Data models,Deep Learning,Generative adversarial networks,Generative Adversarial Networks,Generators,Inference algorithms,Linear programming,Machine learning algorithms,Natural language processing,Theory}
}

@article{haber_stable_2017,
  title = {Stable Architectures for Deep Neural Networks},
  author = {Haber, Eldad and Ruthotto, Lars},
  year = {2017},
  journal = {Inverse problems},
  volume = {34},
  number = {1},
  pages = {014004},
  publisher = {{IOP Publishing}},
  doi = {10.1088/1361-6420/aa9a90}
}

@article{hafner_mastering_2023,
  title = {Mastering {{Diverse Domains}} through {{World Models}}},
  author = {Hafner, Danijar and Pasukonis, Jurgis and Ba, Jimmy and Lillicrap, Timothy},
  year = {2023},
  month = jan,
  number = {arXiv:2301.04104},
  eprint = {2301.04104},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2301.04104},
  url = {http://arxiv.org/abs/2301.04104},
  urldate = {2023-01-18},
  abstract = {General intelligence requires solving tasks across many domains. Current reinforcement learning algorithms carry this potential but are held back by the resources and knowledge required to tune them for new tasks. We present DreamerV3, a general and scalable algorithm based on world models that outperforms previous approaches across a wide range of domains with fixed hyperparameters. These domains include continuous and discrete actions, visual and low-dimensional inputs, 2D and 3D worlds, different data budgets, reward frequencies, and reward scales. We observe favorable scaling properties of DreamerV3, with larger models directly translating to higher data-efficiency and final performance. Applied out of the box, DreamerV3 is the first algorithm to collect diamonds in Minecraft from scratch without human data or curricula, a long-standing challenge in artificial intelligence. Our general algorithm makes reinforcement learning broadly applicable and allows scaling to hard decision-making problems.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  journal = {arXiv:2301.04104}
}

@article{hamilton_inductive_2017,
  title = {Inductive Representation Learning on Large Graphs},
  author = {Hamilton, Will and Ying, Zhitao and Leskovec, Jure},
  year = {2017},
  journal = {Advances in neural information processing systems},
  volume = {30},
  keywords = {⛔ No DOI found}
}

@article{hanebeck_deterministic_2019,
  title = {Deterministic {{Sampling}} of {{Multivariate Densities}} Based on {{Projected Cumulative Distributions}}},
  author = {Hanebeck, Uwe D.},
  year = {2019},
  month = dec,
  number = {arXiv:1912.12875},
  eprint = {1912.12875},
  eprinttype = {arxiv},
  primaryclass = {cs, eess},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1912.12875},
  urldate = {2023-01-27},
  abstract = {We want to approximate general multivariate probability density functions by deterministic sample sets. For optimal sampling, the closeness to the given continuous density has to be assessed. This is a difficult challenge in multivariate settings. Simple solutions are restricted to the one-dimensional case. In this paper, we propose to employ one-dimensional density projections. These are the Radon transforms of the densities. For every projection, we compute their cumulative distribution function. These Projected Cumulative Distributions (PCDs) are compared for all possible projections (or a discrete set thereof). This leads to a tractable distance measure in multivariate space. The proposed approximation method is efficient as calculating the distance measure mainly entails sorting in one dimension. It is also surprisingly simple to implement.},
  archiveprefix = {arXiv},
  keywords = {Electrical Engineering and Systems Science - Systems and Control},
  journal = {arXiv:1912.12875}
}

@article{hanebeck_flux_2018,
  title = {{{FLUX}}: {{Progressive State Estimation Based}} on {{Zakai-type Distributed Ordinary Differential Equations}}},
  shorttitle = {{{FLUX}}},
  author = {Hanebeck, Uwe D.},
  year = {2018},
  month = aug,
  number = {arXiv:1808.02825},
  eprint = {1808.02825},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1808.02825},
  urldate = {2023-01-18},
  abstract = {We propose a homotopy continuation method called FLUX for approximating complicated probability density functions. It is based on progressive processing for smoothly morphing a given density into the desired one. Distributed ordinary differential equations (DODEs) with an artificial time γ ∈ [0, 1] are derived for describing the evolution from the initial density to the desired final density. For a finite-dimensional parametrization, the DODEs are converted to a system of ordinary differential equations (SODEs), which are solved for γ ∈ [0, 1] and return the desired result for γ = 1. This includes parametric representations such as Gaussians or Gaussian mixtures and nonparametric setups such as sample sets. In the latter case, we obtain a particle flow between the two densities along the artificial time.},
  archiveprefix = {arXiv},
  language = {en},
  keywords = {Electrical Engineering and Systems Science - Systems and Control},
  journal = {arXiv:1808.02825}
}

@article{hawizy_chemicaltagger_2011,
  title = {{{ChemicalTagger}}: {{A}} Tool for Semantic Text-Mining in Chemistry},
  shorttitle = {{{ChemicalTagger}}},
  author = {Hawizy, Lezan and Jessop, David M. and Adams, Nico and {Murray-Rust}, Peter},
  year = {2011},
  month = may,
  journal = {Journal of Cheminformatics},
  volume = {3},
  number = {1},
  pages = {17},
  issn = {1758-2946},
  doi = {10.1186/1758-2946-3-17},
  url = {https://doi.org/10.1186/1758-2946-3-17},
  urldate = {2023-02-01},
  abstract = {The primary method for scientific communication is in the form of published scientific articles and theses which use natural language combined with domain-specific terminology. As such, they contain free owing unstructured text. Given the usefulness of data extraction from unstructured literature, we aim to show how this can be achieved for the discipline of chemistry. The highly formulaic style of writing most chemists adopt make their contributions well suited to high-throughput Natural Language Processing (NLP) approaches.},
  keywords = {Atom Transfer Radical Polymerization,Chemical Entity,Dice Coefficient,Natural Language Processing,Potassium Carbonate}
}

@inproceedings{he_deep_2016,
  title = {Deep Residual Learning for Image Recognition},
  booktitle = {Proceedings of the {{IEEE}} Conference on Computer Vision and Pattern Recognition},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  year = {2016},
  pages = {770--778},
  keywords = {⛔ No DOI found}
}

@inproceedings{he_mask_2017,
  title = {Mask R-Cnn},
  booktitle = {Proceedings of the {{IEEE}} International Conference on Computer Vision},
  author = {He, Kaiming and Gkioxari, Georgia and Dollár, Piotr and Girshick, Ross},
  year = {2017},
  pages = {2961--2969},
  keywords = {⛔ No DOI found}
}

@article{hinton_distilling_2015,
  title = {Distilling the {{Knowledge}} in a {{Neural Network}}},
  author = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
  year = {2015},
  month = mar,
  number = {arXiv:1503.02531},
  eprint = {1503.02531},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1503.02531},
  url = {http://arxiv.org/abs/1503.02531},
  urldate = {2023-01-19},
  abstract = {A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  journal = {arXiv:1503.02531}
}

@article{ho_denoising_2020,
  title = {Denoising {{Diffusion Probabilistic Models}}},
  author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
  year = {2020},
  month = dec,
  number = {arXiv:2006.11239},
  eprint = {2006.11239},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2006.11239},
  url = {http://arxiv.org/abs/2006.11239},
  urldate = {2023-01-28},
  abstract = {We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at https://github.com/hojonathanho/diffusion},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  journal = {arXiv:2006.11239}
}

@inproceedings{hoffmann_empirical_2022,
  title = {An Empirical Analysis of Compute-Optimal Large Language Model Training},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and de las Casas, Diego and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and Hennigan, Tom and Noland, Eric and Millican, Katherine and van den Driessche, George and Damoc, Bogdan and Guy, Aurelia and Osindero, Simon and Simonyan, Karen and Elsen, Erich and Vinyals, Oriol and Rae, Jack William and Sifre, Laurent},
  year = {2022},
  month = oct,
  url = {https://openreview.net/forum?id=iBBcRUlOAPR},
  urldate = {2023-01-18},
  abstract = {We investigate the optimal model size and number of tokens for training a transformer language model under a given compute budget. We find that current large language models are significantly undertrained, a consequence of the recent focus on scaling language models whilst keeping the amount of training data constant. By training over 400 language models ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens, we find that for compute-optimal training, the model size and the number of training tokens should be scaled equally: for every doubling of model size the number of training tokens should also be doubled. We test this hypothesis by training a predicted compute-optimal model, Chinchilla, that uses the same compute budget as Gopher but with 70B parameters and 4\$\textbackslash times\$ more data. Chinchilla uniformly and significantly outperformsGopher (280B), GPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of downstream evaluation tasks. This also means that Chinchilla uses substantially less compute for fine-tuning and inference, greatly facilitating downstream usage. As a highlight, Chinchilla reaches a state-of-the-art average accuracy of 67.5\% on the MMLU benchmark, a 7\% improvement over Gopher.},
  language = {en},
  keywords = {DeepMind}
}

@article{hoffmann_training_2022,
  title = {Training {{Compute-Optimal Large Language Models}}},
  author = {Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and Hennigan, Tom and Noland, Eric and Millican, Katie and van den Driessche, George and Damoc, Bogdan and Guy, Aurelia and Osindero, Simon and Simonyan, Karen and Elsen, Erich and Rae, Jack W. and Vinyals, Oriol and Sifre, Laurent},
  year = {2022},
  month = mar,
  number = {arXiv:2203.15556},
  eprint = {2203.15556},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2203.15556},
  url = {http://arxiv.org/abs/2203.15556},
  urldate = {2023-02-06},
  abstract = {We investigate the optimal model size and number of tokens for training a transformer language model under a given compute budget. We find that current large language models are significantly undertrained, a consequence of the recent focus on scaling language models whilst keeping the amount of training data constant. By training over 400 language models ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens, we find that for compute-optimal training, the model size and the number of training tokens should be scaled equally: for every doubling of model size the number of training tokens should also be doubled. We test this hypothesis by training a predicted compute-optimal model, Chinchilla, that uses the same compute budget as Gopher but with 70B parameters and 4\$\textbackslash times\$ more more data. Chinchilla uniformly and significantly outperforms Gopher (280B), GPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of downstream evaluation tasks. This also means that Chinchilla uses substantially less compute for fine-tuning and inference, greatly facilitating downstream usage. As a highlight, Chinchilla reaches a state-of-the-art average accuracy of 67.5\% on the MMLU benchmark, greater than a 7\% improvement over Gopher.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  journal = {arXiv:2203.15556}
}

@article{huang_database_2020,
  title = {A Database of Battery Materials Auto-Generated Using {{ChemDataExtractor}}},
  author = {Huang, Shu and Cole, Jacqueline M.},
  year = {2020},
  month = aug,
  journal = {Scientific Data},
  volume = {7},
  number = {1},
  pages = {260},
  publisher = {{Nature Publishing Group}},
  issn = {2052-4463},
  doi = {10.1038/s41597-020-00602-2},
  url = {https://www.nature.com/articles/s41597-020-00602-2},
  urldate = {2023-02-20},
  abstract = {A database of battery materials is presented which comprises a total of 292,313 data records, with 214,617 unique chemical-property data relations between 17,354 unique chemicals and up to five material properties: capacity, voltage, conductivity, Coulombic efficiency and energy. 117,403 data are multivariate on a property where it is the dependent variable in part of a data series. The database was auto-generated by mining text from 229,061 academic papers using the chemistry-aware natural language processing toolkit, ChemDataExtractor version 1.5, which was modified for the specific domain of batteries. The collected data can be used as a representative overview of battery material information that is contained within text of scientific papers. Public availability of these data will also enable battery materials design and prediction via data-science methods. To the best of our knowledge, this is the first auto-generated database of battery materials extracted from a relatively large number of scientific papers. We also provide a Graphical User Interface (GUI) to aid the use of this database.},
  copyright = {2020 The Author(s)},
  language = {en},
  keywords = {Batteries,Chemical physics,Energy}
}

@article{huber_gaussian_2008,
  title = {Gaussian {{Filter}} Based on {{Deterministic Sampling}} for {{High Quality Nonlinear Estimation}}},
  author = {Huber, Marco F. and Hanebeck, Uwe D.},
  year = {2008},
  journal = {IFAC Proceedings Volumes},
  volume = {41},
  number = {2},
  pages = {13527--13532},
  issn = {14746670},
  doi = {10.3182/20080706-5-KR-1001.02291},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1474667016411572},
  urldate = {2023-01-27},
  abstract = {In this paper, a Gaussian filter for nonlinear Bayesian estimation is introduced that is based on a deterministic sample selection scheme. For an effective sample selection, a parametric density function representation of the sample points is employed, which allows approximating the cumulative distribution function of the prior Gaussian density. The computationally demanding parts of the optimization problem formulated for approximation are carried out off-line for obtaining an efficient filter, whose estimation quality can be altered by adjusting the number of used sample points. The improved performance of the proposed Gaussian filter compared to the well-known unscented Kalman filter is demonstrated by means of two examples.},
  language = {en}
}

@article{jain_autonomy_2021,
  title = {Autonomy 2.0: {{Why}} Is Self-Driving Always 5 Years Away?},
  shorttitle = {Autonomy 2.0},
  author = {Jain, Ashesh and Del Pero, Luca and Grimmett, Hugo and Ondruska, Peter},
  year = {2021},
  journal = {arXiv preprint arXiv:2107.08142},
  eprint = {2107.08142},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  keywords = {⛔ No DOI found}
}

@article{krizhevsky_imagenet_2012,
  title = {Imagenet Classification with Deep Convolutional Neural Networks},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
  year = {2012},
  journal = {Communications of the ACM},
  volume = {60},
  number = {6},
  pages = {84--90},
  publisher = {{AcM New York, NY, USA}},
  doi = {10.1145/3065386}
}

@article{landauer_deep_2022,
  title = {Deep {{Learning}} for {{Anomaly Detection}} in {{Log Data}}: {{A Survey}}},
  shorttitle = {Deep {{Learning}} for {{Anomaly Detection}} in {{Log Data}}},
  author = {Landauer, Max and Onder, Sebastian and Skopik, Florian and Wurzenberger, Markus},
  year = {2022},
  journal = {arXiv preprint arXiv:2207.03820},
  eprint = {2207.03820},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  keywords = {⛔ No DOI found}
}

@article{lecun_convolutional_1995,
  title = {Convolutional Networks for Images, Speech, and Time Series},
  author = {LeCun, Yann and Bengio, Yoshua},
  year = {1995},
  journal = {The handbook of brain theory and neural networks},
  volume = {3361},
  number = {10},
  pages = {1995},
  publisher = {{Cambridge, MA USA}},
  keywords = {⛔ No DOI found}
}

@article{liu_secondorder_2021,
  title = {Second-Order Neural {{ODE}} Optimizer},
  author = {Liu, Guan-Horng and Chen, Tianrong and Theodorou, Evangelos},
  year = {2021},
  journal = {Advances in Neural Information Processing Systems},
  volume = {34},
  pages = {25267--25279},
  keywords = {⛔ No DOI found}
}

@article{lobo_curie_2021,
  title = {{{CURIE}}: A Cellular Automaton for Concept Drift Detection},
  shorttitle = {{{CURIE}}},
  author = {Lobo, Jesus L. and Del Ser, Javier and Osaba, Eneko and Bifet, Albert and Herrera, Francisco},
  year = {2021},
  journal = {Data Mining and Knowledge Discovery},
  volume = {35},
  number = {6},
  pages = {2655--2678},
  publisher = {{Springer}},
  doi = {10.1007/s10618-021-00776-2}
}

@article{lu_learning_2019,
  title = {Learning under {{Concept Drift}}: {{A Review}}},
  shorttitle = {Learning under {{Concept Drift}}},
  author = {Lu, Jie and Liu, Anjin and Dong, Fan and Gu, Feng and Gama, João and Zhang, Guangquan},
  year = {2019},
  month = dec,
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  volume = {31},
  number = {12},
  pages = {2346--2363},
  issn = {1558-2191},
  doi = {10.1109/TKDE.2018.2876857},
  abstract = {Concept drift describes unforeseeable changes in the underlying distribution of streaming data overtime. Concept drift research involves the development of methodologies and techniques for drift detection, understanding, and adaptation. Data analysis has revealed that machine learning in a concept drift environment will result in poor learning results if the drift is not addressed. To help researchers identify which research topics are significant and how to apply related techniques in data analysis tasks, it is necessary that a high quality, instructive review of current research developments and trends in the concept drift field is conducted. In addition, due to the rapid development of concept drift in recent years, the methodologies of learning under concept drift have become noticeably systematic, unveiling a framework which has not been mentioned in literature. This paper reviews over 130 high quality publications in concept drift related research areas, analyzes up-to-date developments in methodologies and techniques, and establishes a framework of learning under concept drift including three main components: concept drift detection, concept drift understanding, and concept drift adaptation. This paper lists and discusses 10 popular synthetic datasets and 14 publicly available benchmark datasets used for evaluating the performance of learning algorithms aiming at handling concept drift. Also, concept drift related research directions are covered and discussed. By providing state-of-the-art knowledge, this survey will directly support researchers in their understanding of research developments in the field of learning under concept drift.},
  keywords = {adaptive learning,Big Data,Cameras,change detection,Concept drift,Data analysis,Data models,data streams,Machine learning,Market research,Mobile handsets}
}

@article{lu_understanding_2019,
  title = {Understanding and {{Improving Transformer From}} a {{Multi-Particle Dynamic System Point}} of {{View}}},
  author = {Lu, Yiping and Li, Zhuohan and He, Di and Sun, Zhiqing and Dong, Bin and Qin, Tao and Wang, Liwei and Liu, Tie-Yan},
  year = {2019},
  month = jun,
  number = {arXiv:1906.02762},
  eprint = {1906.02762},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1906.02762},
  urldate = {2023-01-18},
  abstract = {The Transformer architecture is widely used in natural language processing. Despite its success, the design principle of the Transformer remains elusive. In this paper, we provide a novel perspective towards understanding the architecture: we show that the Transformer can be mathematically interpreted as a numerical Ordinary Differential Equation (ODE) solver for a convection-diffusion equation in a multi-particle dynamic system. In particular, how words in a sentence are abstracted into contexts by passing through the layers of the Transformer can be interpreted as approximating multiple particles' movement in the space using the Lie-Trotter splitting scheme and the Euler's method. Given this ODE's perspective, the rich literature of numerical analysis can be brought to guide us in designing effective structures beyond the Transformer. As an example, we propose to replace the Lie-Trotter splitting scheme by the Strang-Marchuk splitting scheme, a scheme that is more commonly used and with much lower local truncation errors. The Strang-Marchuk splitting scheme suggests that the self-attention and position-wise feed-forward network (FFN) sub-layers should not be treated equally. Instead, in each layer, two position-wise FFN sub-layers should be used, and the self-attention sub-layer is placed in between. This leads to a brand new architecture. Such an FFN-attention-FFN layer is "Macaron-like", and thus we call the network with this new architecture the Macaron Net. Through extensive experiments, we show that the Macaron Net is superior to the Transformer on both supervised and unsupervised learning tasks. The reproducible codes and pretrained models can be found at https://github.com/zhuohan123/macaron-net},
  archiveprefix = {arXiv},
  language = {en},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  journal = {arXiv:1906.02762}
}

@article{luo_mof_2022,
  title = {{{MOF Synthesis Prediction Enabled}} by {{Automatic Data Mining}} and {{Machine Learning}}**},
  author = {Luo, Yi and Bag, Saientan and Zaremba, Orysia and Cierpka, Adrian and Andreo, Jacopo and Wuttke, Stefan and Friederich, Pascal and Tsotsalas, Manuel},
  year = {2022},
  journal = {Angewandte Chemie International Edition},
  volume = {61},
  number = {19},
  pages = {e202200242},
  issn = {1521-3773},
  doi = {10.1002/anie.202200242},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/anie.202200242},
  urldate = {2023-02-01},
  abstract = {Despite rapid progress in the field of metal–organic frameworks (MOFs), the potential of using machine learning (ML) methods to predict MOF synthesis parameters is still untapped. Here, we show how ML can be used for rationalization and acceleration of the MOF discovery process by directly predicting the synthesis conditions of a MOF based on its crystal structure. Our approach is based on: i) establishing the first MOF synthesis database via automatic extraction of synthesis parameters from the literature, ii) training and optimizing ML models by employing the MOF database, and iii) predicting the synthesis conditions for new MOF structures. The ML models, even at an initial stage, exhibit a good prediction performance, outperforming human expert predictions, obtained through a synthesis survey. The automated synthesis prediction is available via a web-tool on https://mof-synthesis.aimat.science.},
  language = {en},
  keywords = {Data Mining,Machine Learning,Metal–Organic Frameworks,Microporous Materials,Synthesis Prediction},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/anie.202200242}
}

@article{madani_large_2023,
  title = {Large Language Models Generate Functional Protein Sequences across Diverse Families},
  author = {Madani, Ali and Krause, Ben and Greene, Eric R. and Subramanian, Subu and Mohr, Benjamin P. and Holton, James M. and Olmos, Jose Luis and Xiong, Caiming and Sun, Zachary Z. and Socher, Richard and Fraser, James S. and Naik, Nikhil},
  year = {2023},
  month = jan,
  journal = {Nature Biotechnology},
  pages = {1--8},
  publisher = {{Nature Publishing Group}},
  issn = {1546-1696},
  doi = {10.1038/s41587-022-01618-2},
  url = {https://www.nature.com/articles/s41587-022-01618-2},
  urldate = {2023-02-16},
  abstract = {Deep-learning language models have shown promise in various biotechnological applications, including protein design and engineering. Here we describe ProGen, a language model that can generate protein sequences with a predictable function across large protein families, akin to generating grammatically and semantically correct natural language sentences on diverse topics. The model was trained on 280 million protein sequences from {$>$}19,000 families and is augmented with control tags specifying protein properties. ProGen can be further fine-tuned to curated sequences and tags to improve controllable generation performance of proteins from families with sufficient homologous samples. Artificial proteins fine-tuned to five distinct lysozyme families showed similar catalytic efficiencies as natural lysozymes, with sequence identity to natural proteins as low as 31.4\%. ProGen is readily adapted to diverse protein families, as we demonstrate with chorismate mutase and malate dehydrogenase.},
  copyright = {2023 The Author(s), under exclusive licence to Springer Nature America, Inc.},
  language = {en},
  keywords = {Enzymes,Machine learning,Proteomics}
}

@article{metz_unrolled_2016,
  title = {Unrolled Generative Adversarial Networks},
  author = {Metz, Luke and Poole, Ben and Pfau, David and {Sohl-Dickstein}, Jascha},
  year = {2016},
  journal = {arXiv preprint arXiv:1611.02163},
  eprint = {1611.02163},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  keywords = {⛔ No DOI found,GAN}
}

@article{mildenhall_nerf_2020,
  title = {{{NeRF}}: {{Representing Scenes}} as {{Neural Radiance Fields}} for {{View Synthesis}}},
  shorttitle = {{{NeRF}}},
  author = {Mildenhall, Ben and Srinivasan, Pratul P. and Tancik, Matthew and Barron, Jonathan T. and Ramamoorthi, Ravi and Ng, Ren},
  year = {2020},
  month = aug,
  number = {arXiv:2003.08934},
  eprint = {2003.08934},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2003.08934},
  url = {http://arxiv.org/abs/2003.08934},
  urldate = {2023-01-26},
  abstract = {We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location \$(x,y,z)\$ and viewing direction \$(\textbackslash theta, \textbackslash phi)\$) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics},
  journal = {arXiv:2003.08934}
}

@article{millidge_neural_2021,
  title = {Neural {{Kalman Filtering}}},
  author = {Millidge, Beren and Tschantz, Alexander and Seth, Anil and Buckley, Christopher},
  year = {2021},
  month = apr,
  number = {arXiv:2102.10021},
  eprint = {2102.10021},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2102.10021},
  url = {http://arxiv.org/abs/2102.10021},
  urldate = {2023-01-19},
  abstract = {The Kalman filter is a fundamental filtering algorithm that fuses noisy sensory data, a previous state estimate, and a dynamics model to produce a principled estimate of the current state. It assumes, and is optimal for, linear models and white Gaussian noise. Due to its relative simplicity and general effectiveness, the Kalman filter is widely used in engineering applications. Since many sensory problems the brain faces are, at their core, filtering problems, it is possible that the brain possesses neural circuitry that implements equivalent computations to the Kalman filter. The standard approach to Kalman filtering requires complex matrix computations that are unlikely to be directly implementable in neural circuits. In this paper, we show that a gradient-descent approximation to the Kalman filter requires only local computations with variance weighted prediction errors. Moreover, we show that it is possible under the same scheme to adaptively learn the dynamics model with a learning rule that corresponds directly to Hebbian plasticity. We demonstrate the performance of our method on a simple Kalman filtering task, and propose a neural implementation of the required equations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Neural and Evolutionary Computing},
  journal = {arXiv:2102.10021}
}

@article{norcliffe_neural_2021,
  title = {Neural {{ODE Processes}}},
  author = {Norcliffe, Alexander and Bodnar, Cristian and Day, Ben and Moss, Jacob and Liò, Pietro},
  year = {2021},
  month = aug,
  number = {arXiv:2103.12413},
  eprint = {2103.12413},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2103.12413},
  urldate = {2023-01-28},
  abstract = {Neural Ordinary Differential Equations (NODEs) use a neural network to model the instantaneous rate of change in the state of a system. However, despite their apparent suitability for dynamics-governed time-series, NODEs present a few disadvantages. First, they are unable to adapt to incoming data points, a fundamental requirement for real-time applications imposed by the natural direction of time. Second, time series are often composed of a sparse set of measurements that could be explained by many possible underlying dynamics. NODEs do not capture this uncertainty. In contrast, Neural Processes (NPs) are a family of models providing uncertainty estimation and fast data adaptation but lack an explicit treatment of the flow of time. To address these problems, we introduce Neural ODE Processes (NDPs), a new class of stochastic processes determined by a distribution over Neural ODEs. By maintaining an adaptive data-dependent distribution over the underlying ODE, we show that our model can successfully capture the dynamics of low-dimensional systems from just a few data points. At the same time, we demonstrate that NDPs scale up to challenging high-dimensional time-series with unknown latent dynamics such as rotating MNIST digits.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  journal = {arXiv:2103.12413}
}

@article{ott_when_2020,
  title = {When Are Neural {{ODE}} Solutions Proper {{ODEs}}?},
  author = {Ott, Katharina and Katiyar, Prateek and Hennig, Philipp and Tiemann, Michael},
  year = {2020},
  journal = {arXiv preprint arXiv:2007.15386},
  eprint = {2007.15386},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  keywords = {⛔ No DOI found}
}

@article{ouyang_training_2022,
  title = {Training Language Models to Follow Instructions with Human Feedback},
  author = {Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L. and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul and Leike, Jan and Lowe, Ryan},
  year = {2022},
  month = mar,
  number = {arXiv:2203.02155},
  eprint = {2203.02155},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2203.02155},
  url = {http://arxiv.org/abs/2203.02155},
  urldate = {2023-02-16},
  abstract = {Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  journal = {arXiv:2203.02155}
}

@article{polino_model_2018,
  title = {Model Compression via Distillation and Quantization},
  author = {Polino, Antonio and Pascanu, Razvan and Alistarh, Dan},
  year = {2018},
  month = feb,
  number = {arXiv:1802.05668},
  eprint = {1802.05668},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1802.05668},
  url = {http://arxiv.org/abs/1802.05668},
  urldate = {2023-01-19},
  abstract = {Deep neural networks (DNNs) continue to make significant advances, solving tasks from image classification to translation or reinforcement learning. One aspect of the field receiving considerable attention is efficiently executing deep models in resource-constrained environments, such as mobile or embedded devices. This paper focuses on this problem, and proposes two new compression methods, which jointly leverage weight quantization and distillation of larger teacher networks into smaller student networks. The first method we propose is called quantized distillation and leverages distillation during the training process, by incorporating distillation loss, expressed with respect to the teacher, into the training of a student network whose weights are quantized to a limited set of levels. The second method, differentiable quantization, optimizes the location of quantization points through stochastic gradient descent, to better fit the behavior of the teacher model. We validate both methods through experiments on convolutional and recurrent architectures. We show that quantized shallow students can reach similar accuracy levels to full-precision teacher models, while providing order of magnitude compression, and inference speedup that is linear in the depth reduction. In sum, our results enable DNNs for resource-constrained environments to leverage architecture and accuracy advances developed on more powerful devices.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  journal = {arXiv:1802.05668}
}

@inproceedings{prossel_dirac_2022,
  title = {Dirac {{Mixture Reduction Using Wasserstein Distances}} on {{Projected Cumulative Distributions}}},
  booktitle = {2022 25th {{International Conference}} on {{Information Fusion}} ({{FUSION}})},
  author = {Prossel, Dominik and Hanebeck, Uwe D.},
  year = {2022},
  month = jul,
  pages = {1--8},
  publisher = {{IEEE}},
  address = {{Linköping, Sweden}},
  doi = {10.23919/FUSION49751.2022.9841286},
  url = {https://ieeexplore.ieee.org/document/9841286/},
  urldate = {2023-01-18},
  abstract = {The reapproximation of discrete probability densities is a common task in sample-based filters such as the particle filter. It can be viewed as the approximation of a given Dirac mixture density with another one, typically with fewer samples. In this paper, the Wasserstein distance is established as a suitable measure to compare two Dirac mixtures. The resulting minimization problem is also known as location-allocation or facility location problem and cannot be solved in polynomial time. Therefore, the well-known sliced Wasserstein distance is introduced as a replacement and its ties to the projected cumulative distribution (PCD) are shown. An iterative algorithm is proposed to minimize the sliced Wasserstein distance between the given distribution and approximation.},
  isbn = {978-1-73774-972-1},
  language = {en}
}

@article{qi_pointnet_2017,
  title = {Pointnet++: {{Deep}} Hierarchical Feature Learning on Point Sets in a Metric Space},
  shorttitle = {Pointnet++},
  author = {Qi, Charles Ruizhongtai and Yi, Li and Su, Hao and Guibas, Leonidas J.},
  year = {2017},
  journal = {Advances in neural information processing systems},
  volume = {30},
  keywords = {⛔ No DOI found}
}

@inproceedings{qi_pointnet_2017a,
  title = {Pointnet: {{Deep}} Learning on Point Sets for 3d Classification and Segmentation},
  shorttitle = {Pointnet},
  booktitle = {Proceedings of the {{IEEE}} Conference on Computer Vision and Pattern Recognition},
  author = {Qi, Charles R. and Su, Hao and Mo, Kaichun and Guibas, Leonidas J.},
  year = {2017},
  pages = {652--660},
  keywords = {⛔ No DOI found}
}

@article{qiu_accuracy_2021,
  title = {Accuracy and {{Architecture Studies}} of {{Residual Neural Network}} Solving {{Ordinary Differential Equations}}},
  author = {Qiu, Changxin and Bendickson, Aaron and Kalyanapu, Joshua and Yan, Jue},
  year = {2021},
  journal = {arXiv preprint arXiv:2101.03583},
  eprint = {2101.03583},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  keywords = {⛔ No DOI found}
}

@article{radford_improving_2018,
  title = {Improving {{Language Understanding}} by {{Generative Pre-Training}}},
  author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  year = {2018},
  abstract = {Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, semantic similarity assessment, and document classification. Although large unlabeled text corpora are abundant, labeled data for learning these specific tasks is scarce, making it challenging for discriminatively trained models to perform adequately. We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative fine-tuning on each specific task. In contrast to previous approaches, we make use of task-aware input transformations during fine-tuning to achieve effective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our approach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms discriminatively trained models that use architectures specifically crafted for each task, significantly improving upon the state of the art in 9 out of the 12 tasks studied. For instance, we achieve absolute improvements of 8.9\% on commonsense reasoning (Stories Cloze Test), 5.7\% on question answering (RACE), and 1.5\% on textual entailment (MultiNLI).},
  language = {en},
  keywords = {⛔ No DOI found}
}

@article{radford_language_2019,
  title = {Language {{Models}} Are {{Unsupervised Multitask Learners}}},
  author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year = {2019},
  journal = {published on GitHub},
  abstract = {Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset - matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.},
  language = {en},
  keywords = {⛔ No DOI found}
}

@article{radford_learning_2021,
  title = {Learning {{Transferable Visual Models From Natural Language Supervision}}},
  author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
  year = {2021},
  month = feb,
  number = {arXiv:2103.00020},
  eprint = {2103.00020},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2103.00020},
  url = {http://arxiv.org/abs/2103.00020},
  urldate = {2023-02-18},
  abstract = {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  journal = {arXiv:2103.00020}
}

@article{raissi_multistep_2018,
  title = {Multistep Neural Networks for Data-Driven Discovery of Nonlinear Dynamical Systems},
  author = {Raissi, Maziar and Perdikaris, Paris and Karniadakis, George Em},
  year = {2018},
  journal = {arXiv preprint arXiv:1801.01236},
  eprint = {1801.01236},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  keywords = {⛔ No DOI found}
}

@inproceedings{ramesh_zeroshot_2021,
  title = {Zero-{{Shot Text-to-Image Generation}}},
  booktitle = {Proceedings of the 38th {{International Conference}} on {{Machine Learning}}},
  author = {Ramesh, Aditya and Pavlov, Mikhail and Goh, Gabriel and Gray, Scott and Voss, Chelsea and Radford, Alec and Chen, Mark and Sutskever, Ilya},
  year = {2021},
  month = jul,
  pages = {8821--8831},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v139/ramesh21a.html},
  urldate = {2023-02-04},
  abstract = {Text-to-image generation has traditionally focused on finding better modeling assumptions for training on a fixed dataset. These assumptions might involve complex architectures, auxiliary losses, or side information such as object part labels or segmentation masks supplied during training. We describe a simple approach for this task based on a transformer that autoregressively models the text and image tokens as a single stream of data. With sufficient data and scale, our approach is competitive with previous domain-specific models when evaluated in a zero-shot fashion.},
  language = {en}
}

@article{ren_faster_2015,
  title = {Faster R-Cnn: {{Towards}} Real-Time Object Detection with Region Proposal Networks},
  shorttitle = {Faster R-Cnn},
  author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
  year = {2015},
  journal = {Advances in neural information processing systems},
  volume = {28},
  keywords = {⛔ No DOI found}
}

@book{roberts_principles_2022,
  title = {The {{Principles}} of {{Deep Learning Theory}}},
  author = {Roberts, Daniel A. and Yaida, Sho and Hanin, Boris},
  year = {2022},
  month = may,
  eprint = {2106.10165},
  eprinttype = {arxiv},
  primaryclass = {hep-th, stat},
  doi = {10.1017/9781009023405},
  url = {http://arxiv.org/abs/2106.10165},
  urldate = {2023-02-09},
  abstract = {This book develops an effective theory approach to understanding deep neural networks of practical relevance. Beginning from a first-principles component-level picture of networks, we explain how to determine an accurate description of the output of trained networks by solving layer-to-layer iteration equations and nonlinear learning dynamics. A main result is that the predictions of networks are described by nearly-Gaussian distributions, with the depth-to-width aspect ratio of the network controlling the deviations from the infinite-width Gaussian description. We explain how these effectively-deep networks learn nontrivial representations from training and more broadly analyze the mechanism of representation learning for nonlinear models. From a nearly-kernel-methods perspective, we find that the dependence of such models' predictions on the underlying learning algorithm can be expressed in a simple and universal way. To obtain these results, we develop the notion of representation group flow (RG flow) to characterize the propagation of signals through the network. By tuning networks to criticality, we give a practical solution to the exploding and vanishing gradient problem. We further explain how RG flow leads to near-universal behavior and lets us categorize networks built from different activation functions into universality classes. Altogether, we show that the depth-to-width ratio governs the effective model complexity of the ensemble of trained networks. By using information-theoretic techniques, we estimate the optimal aspect ratio at which we expect the network to be practically most useful and show how residual connections can be used to push this scale to arbitrary depths. With these tools, we can learn in detail about the inductive bias of architectures, hyperparameters, and optimizers.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,High Energy Physics - Theory,Statistics - Machine Learning},
  journal = {arXiv:2106.10165}
}

@article{ruoff_progressive,
  title = {Progressive Correction for Deterministic {{Dirac}} Mixture Approximations},
  author = {Ruoff, Patrick and Krauthausen, Peter and Hanebeck, Uwe D},
  abstract = {Since the advent of Monte-Carlo particle filtering, particle representations of densities have become increasingly popular due to their flexibility and implicit adaptive resolution. In this paper, an algorithm for the multiplication of a systematic Dirac mixture (DM) approximation with a continuous likelihood function is presented, which applies a progressive correction scheme, in order to avoid the particle degeneration problem. The preservation of sample regularity and therefore, representation quality of the underlying smooth density, is ensured by including a new measure of smoothness for Dirac mixtures, the DM energy, into the distance measure. A comparison to common correction schemes in Monte-Carlo methods reveals large improvements especially in cases of small overlap between the likelihood and prior density, as well as for multi-modal likelihoods.},
  language = {en},
  keywords = {⛔ No DOI found}
}

@article{ruthotto_deep_2020,
  title = {Deep Neural Networks Motivated by Partial Differential Equations},
  author = {Ruthotto, Lars and Haber, Eldad},
  year = {2020},
  journal = {Journal of Mathematical Imaging and Vision},
  volume = {62},
  number = {3},
  pages = {352--364},
  publisher = {{Springer}},
  doi = {10.1007/s10851-019-00903-1}
}

@article{schick_toolformer_2023,
  title = {Toolformer: {{Language Models Can Teach Themselves}} to {{Use Tools}}},
  shorttitle = {Toolformer},
  author = {Schick, Timo and {Dwivedi-Yu}, Jane and Dessì, Roberto and Raileanu, Roberta and Lomeli, Maria and Zettlemoyer, Luke and Cancedda, Nicola and Scialom, Thomas},
  year = {2023},
  month = feb,
  number = {arXiv:2302.04761},
  eprint = {2302.04761},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2302.04761},
  url = {http://arxiv.org/abs/2302.04761},
  urldate = {2023-02-11},
  abstract = {Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q\textbackslash\&A system, two different search engines, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  journal = {arXiv:2302.04761}
}

@article{schmidl_anomaly_2022,
  title = {Anomaly Detection in Time Series: A Comprehensive Evaluation},
  shorttitle = {Anomaly Detection in Time Series},
  author = {Schmidl, Sebastian and Wenig, Phillip and Papenbrock, Thorsten},
  year = {2022},
  journal = {Proceedings of the VLDB Endowment},
  volume = {15},
  number = {9},
  pages = {1779--1797},
  publisher = {{VLDB Endowment}},
  doi = {10.14778/3538598.3538602}
}

@article{shi_automated_2021,
  title = {Automated Experimentation Powers Data Science in Chemistry},
  author = {Shi, Yao and Prieto, Paloma L. and Zepel, Tara and Grunert, Shad and Hein, Jason E.},
  year = {2021},
  journal = {Accounts of Chemical Research},
  volume = {54},
  number = {3},
  pages = {546--555},
  publisher = {{ACS Publications}},
  doi = {10.1021/acs.accounts.0c00736}
}

@article{soleimani_scalable_2017,
  title = {Scalable Joint Models for Reliable Uncertainty-Aware Event Prediction},
  author = {Soleimani, Hossein and Hensman, James and Saria, Suchi},
  year = {2017},
  journal = {IEEE transactions on pattern analysis and machine intelligence},
  volume = {40},
  number = {8},
  pages = {1948--1963},
  publisher = {{IEEE}},
  doi = {10.1109/TPAMI.2017.2742504}
}

@article{soleimani_treatmentresponse_2017,
  title = {Treatment-Response Models for Counterfactual Reasoning with Continuous-Time, Continuous-Valued Interventions},
  author = {Soleimani, Hossein and Subbaswamy, Adarsh and Saria, Suchi},
  year = {2017},
  journal = {arXiv preprint arXiv:1704.02038},
  eprint = {1704.02038},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  keywords = {⛔ No DOI found}
}

@article{srivastava_dropout_2014,
  title = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
  shorttitle = {Dropout},
  author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  year = {2014},
  journal = {The journal of machine learning research},
  volume = {15},
  number = {1},
  pages = {1929--1958},
  publisher = {{JMLR. org}},
  keywords = {⛔ No DOI found}
}

@article{sun_patient_2019,
  title = {Patient {{Knowledge Distillation}} for {{BERT Model Compression}}},
  author = {Sun, Siqi and Cheng, Yu and Gan, Zhe and Liu, Jingjing},
  year = {2019},
  month = aug,
  number = {arXiv:1908.09355},
  eprint = {1908.09355},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1908.09355},
  urldate = {2023-02-13},
  abstract = {Pre-trained language models such as BERT have proven to be highly effective for natural language processing (NLP) tasks. However, the high demand for computing resources in training such models hinders their application in practice. In order to alleviate this resource hunger in large-scale model training, we propose a Patient Knowledge Distillation approach to compress an original large model (teacher) into an equally-effective lightweight shallow network (student). Different from previous knowledge distillation methods, which only use the output from the last layer of the teacher network for distillation, our student model patiently learns from multiple intermediate layers of the teacher model for incremental knowledge extraction, following two strategies: (\$i\$) PKD-Last: learning from the last \$k\$ layers; and (\$ii\$) PKD-Skip: learning from every \$k\$ layers. These two patient distillation schemes enable the exploitation of rich information in the teacher's hidden layers, and encourage the student model to patiently learn from and imitate the teacher through a multi-layer distillation process. Empirically, this translates into improved results on multiple NLP tasks with significant gain in training efficiency, without sacrificing model accuracy.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  journal = {arXiv:1908.09355}
}

@article{tian_mononerf_2022,
  title = {{{MonoNeRF}}: {{Learning}} a {{Generalizable Dynamic Radiance Field}} from {{Monocular Videos}}},
  shorttitle = {{{MonoNeRF}}},
  author = {Tian, Fengrui and Du, Shaoyi and Duan, Yueqi},
  year = {2022},
  month = dec,
  number = {arXiv:2212.13056},
  eprint = {2212.13056},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2212.13056},
  url = {http://arxiv.org/abs/2212.13056},
  urldate = {2023-01-18},
  abstract = {In this paper, we target at the problem of learning a generalizable dynamic radiance field from monocular videos. Different from most existing NeRF methods that are based on multiple views, monocular videos only contain one view at each timestamp, thereby suffering from ambiguity along the view direction in estimating point features and scene flows. Previous studies such as DynNeRF disambiguate point features by positional encoding, which is not transferable and severely limits the generalization ability. As a result, these methods have to train one independent model for each scene and suffer from heavy computational costs when applying to increasing monocular videos in real-world applications. To address this, We propose MonoNeRF to simultaneously learn point features and scene flows with point trajectory and feature correspondence constraints across frames. More specifically, we learn an implicit velocity field to estimate point trajectory from temporal features with Neural ODE, which is followed by a flow-based feature aggregation module to obtain spatial features along the point trajectory. We jointly optimize temporal and spatial features by training the network in an end-to-end manner. Experiments show that our MonoNeRF is able to learn from multiple scenes and support new applications such as scene editing, unseen frame synthesis, and fast novel scene adaptation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  journal = {arXiv:2212.13056}
}

@article{vaswani_attention_2017,
  title = {Attention Is All You Need},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, \textbackslash Lukasz and Polosukhin, Illia},
  year = {2017},
  journal = {Advances in neural information processing systems},
  volume = {30},
  keywords = {⛔ No DOI found}
}

@article{wiewel_latent_2019,
  title = {Latent {{Space Physics}}: {{Towards Learning}} the {{Temporal Evolution}} of {{Fluid Flow}}},
  shorttitle = {Latent {{Space Physics}}},
  author = {Wiewel, S. and Becher, M. and Thuerey, N.},
  year = {2019},
  journal = {Computer Graphics Forum},
  volume = {38},
  number = {2},
  pages = {71--82},
  issn = {1467-8659},
  doi = {10.1111/cgf.13620},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.13620},
  urldate = {2023-01-19},
  abstract = {We propose a method for the data-driven inference of temporal evolutions of physical functions with deep learning. More specifically, we target fluid flow problems, and we propose a novel LSTM-based approach to predict the changes of the pressure field over time. The central challenge in this context is the high dimensionality of Eulerian space-time data sets. We demonstrate for the first time that dense 3D+time functions of physics system can be predicted within the latent spaces of neural networks, and we arrive at a neural-network based simulation algorithm with significant practical speed-ups. We highlight the capabilities of our method with a series of complex liquid simulations, and with a set of single-phase buoyancy simulations. With a set of trained networks, our method is more than two orders of magnitudes faster than a traditional pressure solver. Additionally, we present and discuss a series of detailed evaluations for the different components of our algorithm.},
  language = {en},
  keywords = {• Computing methodologies → Neural networks,CCS Concepts,Physical simulation},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/cgf.13620}
}

@article{workshop_bloom_2022,
  title = {{{BLOOM}}: {{A 176B-Parameter Open-Access Multilingual Language Model}}},
  shorttitle = {{{BLOOM}}},
  author = {Workshop, BigScience and Scao, Teven Le and Fan, Angela and Akiki, Christopher and Pavlick, Ellie and Ilić, Suzana and Hesslow, Daniel and Castagné, Roman and Luccioni, Alexandra Sasha and Yvon, François and Gallé, Matthias and Tow, Jonathan and Rush, Alexander M. and Biderman, Stella and Webson, Albert and Ammanamanchi, Pawan Sasanka and Wang, Thomas and Sagot, Benoît and Muennighoff, Niklas and {del Moral}, Albert Villanova and Ruwase, Olatunji and Bawden, Rachel and Bekman, Stas and {McMillan-Major}, Angelina and Beltagy, Iz and Nguyen, Huu and Saulnier, Lucile and Tan, Samson and Suarez, Pedro Ortiz and Sanh, Victor and Laurençon, Hugo and Jernite, Yacine and Launay, Julien and Mitchell, Margaret and Raffel, Colin and Gokaslan, Aaron and Simhi, Adi and Soroa, Aitor and Aji, Alham Fikri and Alfassy, Amit and Rogers, Anna and Nitzav, Ariel Kreisberg and Xu, Canwen and Mou, Chenghao and Emezue, Chris and Klamm, Christopher and Leong, Colin and {van Strien}, Daniel and Adelani, David Ifeoluwa and Radev, Dragomir and Ponferrada, Eduardo González and Levkovizh, Efrat and Kim, Ethan and Natan, Eyal Bar and De Toni, Francesco and Dupont, Gérard and Kruszewski, Germán and Pistilli, Giada and Elsahar, Hady and Benyamina, Hamza and Tran, Hieu and Yu, Ian and Abdulmumin, Idris and Johnson, Isaac and {Gonzalez-Dios}, Itziar and {de la Rosa}, Javier and Chim, Jenny and Dodge, Jesse and Zhu, Jian and Chang, Jonathan and Frohberg, Jörg and Tobing, Joseph and Bhattacharjee, Joydeep and Almubarak, Khalid and Chen, Kimbo and Lo, Kyle and Von Werra, Leandro and Weber, Leon and Phan, Long and {allal}, Loubna Ben and Tanguy, Ludovic and Dey, Manan and Muñoz, Manuel Romero and Masoud, Maraim and Grandury, María and Šaško, Mario and Huang, Max and Coavoux, Maximin and Singh, Mayank and Jiang, Mike Tian-Jian and Vu, Minh Chien and Jauhar, Mohammad A. and Ghaleb, Mustafa and Subramani, Nishant and Kassner, Nora and Khamis, Nurulaqilla and Nguyen, Olivier and Espejel, Omar and {de Gibert}, Ona and Villegas, Paulo and Henderson, Peter and Colombo, Pierre and Amuok, Priscilla and Lhoest, Quentin and Harliman, Rheza and Bommasani, Rishi and López, Roberto Luis and Ribeiro, Rui and Osei, Salomey and Pyysalo, Sampo and Nagel, Sebastian and Bose, Shamik and Muhammad, Shamsuddeen Hassan and Sharma, Shanya and Longpre, Shayne and Nikpoor, Somaieh and Silberberg, Stanislav and Pai, Suhas and Zink, Sydney and Torrent, Tiago Timponi and Schick, Timo and Thrush, Tristan and Danchev, Valentin and Nikoulina, Vassilina and Laippala, Veronika and Lepercq, Violette and Prabhu, Vrinda and Alyafeai, Zaid and Talat, Zeerak and Raja, Arun and Heinzerling, Benjamin and Si, Chenglei and Taşar, Davut Emre and Salesky, Elizabeth and Mielke, Sabrina J. and Lee, Wilson Y. and Sharma, Abheesht and Santilli, Andrea and Chaffin, Antoine and Stiegler, Arnaud and Datta, Debajyoti and Szczechla, Eliza and Chhablani, Gunjan and Wang, Han and Pandey, Harshit and Strobelt, Hendrik and Fries, Jason Alan and Rozen, Jos and Gao, Leo and Sutawika, Lintang and Bari, M. Saiful and {Al-shaibani}, Maged S. and Manica, Matteo and Nayak, Nihal and Teehan, Ryan and Albanie, Samuel and Shen, Sheng and {Ben-David}, Srulik and Bach, Stephen H. and Kim, Taewoon and Bers, Tali and Fevry, Thibault and Neeraj, Trishala and Thakker, Urmish and Raunak, Vikas and Tang, Xiangru and Yong, Zheng-Xin and Sun, Zhiqing and Brody, Shaked and Uri, Yallow and Tojarieh, Hadar and Roberts, Adam and Chung, Hyung Won and Tae, Jaesung and Phang, Jason and Press, Ofir and Li, Conglong and Narayanan, Deepak and Bourfoune, Hatim and Casper, Jared and Rasley, Jeff and Ryabinin, Max and Mishra, Mayank and Zhang, Minjia and Shoeybi, Mohammad and Peyrounette, Myriam and Patry, Nicolas and Tazi, Nouamane and Sanseviero, Omar and {von Platen}, Patrick and Cornette, Pierre and Lavallée, Pierre François and Lacroix, Rémi and Rajbhandari, Samyam and Gandhi, Sanchit and Smith, Shaden and Requena, Stéphane and Patil, Suraj and Dettmers, Tim and Baruwa, Ahmed and Singh, Amanpreet and Cheveleva, Anastasia and Ligozat, Anne-Laure and Subramonian, Arjun and Névéol, Aurélie and Lovering, Charles and Garrette, Dan and Tunuguntla, Deepak and Reiter, Ehud and Taktasheva, Ekaterina and Voloshina, Ekaterina and Bogdanov, Eli and Winata, Genta Indra and Schoelkopf, Hailey and Kalo, Jan-Christoph and Novikova, Jekaterina and Forde, Jessica Zosa and Clive, Jordan and Kasai, Jungo and Kawamura, Ken and Hazan, Liam and Carpuat, Marine and Clinciu, Miruna and Kim, Najoung and Cheng, Newton and Serikov, Oleg and Antverg, Omer and {van der Wal}, Oskar and Zhang, Rui and Zhang, Ruochen and Gehrmann, Sebastian and Mirkin, Shachar and Pais, Shani and Shavrina, Tatiana and Scialom, Thomas and Yun, Tian and Limisiewicz, Tomasz and Rieser, Verena and Protasov, Vitaly and Mikhailov, Vladislav and Pruksachatkun, Yada and Belinkov, Yonatan and Bamberger, Zachary and Kasner, Zdeněk and Rueda, Alice and Pestana, Amanda and Feizpour, Amir and Khan, Ammar and Faranak, Amy and Santos, Ana and Hevia, Anthony and Unldreaj, Antigona and Aghagol, Arash and Abdollahi, Arezoo and Tammour, Aycha and HajiHosseini, Azadeh and Behroozi, Bahareh and Ajibade, Benjamin and Saxena, Bharat and Ferrandis, Carlos Muñoz and Contractor, Danish and Lansky, David and David, Davis and Kiela, Douwe and Nguyen, Duong A. and Tan, Edward and Baylor, Emi and Ozoani, Ezinwanne and Mirza, Fatima and Ononiwu, Frankline and Rezanejad, Habib and Jones, Hessie and Bhattacharya, Indrani and Solaiman, Irene and Sedenko, Irina and Nejadgholi, Isar and Passmore, Jesse and Seltzer, Josh and Sanz, Julio Bonis and Dutra, Livia and Samagaio, Mairon and Elbadri, Maraim and Mieskes, Margot and Gerchick, Marissa and Akinlolu, Martha and McKenna, Michael and Qiu, Mike and Ghauri, Muhammed and Burynok, Mykola and Abrar, Nafis and Rajani, Nazneen and Elkott, Nour and Fahmy, Nour and Samuel, Olanrewaju and An, Ran and Kromann, Rasmus and Hao, Ryan and Alizadeh, Samira and Shubber, Sarmad and Wang, Silas and Roy, Sourav and Viguier, Sylvain and Le, Thanh and Oyebade, Tobi and Le, Trieu and Yang, Yoyo and Nguyen, Zach and Kashyap, Abhinav Ramesh and Palasciano, Alfredo and Callahan, Alison and Shukla, Anima and {Miranda-Escalada}, Antonio and Singh, Ayush and Beilharz, Benjamin and Wang, Bo and Brito, Caio and Zhou, Chenxi and Jain, Chirag and Xu, Chuxin and Fourrier, Clémentine and Periñán, Daniel León and Molano, Daniel and Yu, Dian and Manjavacas, Enrique and Barth, Fabio and Fuhrimann, Florian and Altay, Gabriel and Bayrak, Giyaseddin and Burns, Gully and Vrabec, Helena U. and Bello, Imane and Dash, Ishani and Kang, Jihyun and Giorgi, John and Golde, Jonas and Posada, Jose David and Sivaraman, Karthik Rangasai and Bulchandani, Lokesh and Liu, Lu and Shinzato, Luisa and {de Bykhovetz}, Madeleine Hahn and Takeuchi, Maiko and Pàmies, Marc and Castillo, Maria A. and Nezhurina, Marianna and Sänger, Mario and Samwald, Matthias and Cullan, Michael and Weinberg, Michael and De Wolf, Michiel and Mihaljcic, Mina and Liu, Minna and Freidank, Moritz and Kang, Myungsun and Seelam, Natasha and Dahlberg, Nathan and Broad, Nicholas Michio and Muellner, Nikolaus and Fung, Pascale and Haller, Patrick and Chandrasekhar, Ramya and Eisenberg, Renata and Martin, Robert and Canalli, Rodrigo and Su, Rosaline and Su, Ruisi and Cahyawijaya, Samuel and Garda, Samuele and Deshmukh, Shlok S. and Mishra, Shubhanshu and Kiblawi, Sid and Ott, Simon and {Sang-aroonsiri}, Sinee and Kumar, Srishti and Schweter, Stefan and Bharati, Sushil and Laud, Tanmay and Gigant, Théo and Kainuma, Tomoya and Kusa, Wojciech and Labrak, Yanis and Bajaj, Yash Shailesh and Venkatraman, Yash and Xu, Yifan and Xu, Yingxin and Xu, Yu and Tan, Zhe and Xie, Zhongli and Ye, Zifan and Bras, Mathilde and Belkada, Younes and Wolf, Thomas},
  year = {2022},
  month = dec,
  number = {arXiv:2211.05100},
  eprint = {2211.05100},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2211.05100},
  url = {http://arxiv.org/abs/2211.05100},
  urldate = {2023-02-16},
  abstract = {Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  journal = {arXiv:2211.05100}
}

@article{xu_hyperspectral_2022,
  title = {Hyperspectral {{Anomaly Detection Based}} on {{Machine Learning}}: {{An Overview}}},
  shorttitle = {Hyperspectral {{Anomaly Detection Based}} on {{Machine Learning}}},
  author = {Xu, Yichu and Zhang, Lefei and Du, Bo and Zhang, Liangpei},
  year = {2022},
  journal = {IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
  volume = {15},
  pages = {3351--3364},
  issn = {2151-1535},
  doi = {10.1109/JSTARS.2022.3167830},
  abstract = {Hyperspectral anomaly detection (HAD) is an important hyperspectral image application. HAD can find pixels with anomalous spectral signatures compared with their neighbor background without any prior information. While most of the existed researches are related to statistic-based and distance-based techniques, by summarizing the background samples with certain models, and then, finding the very few outliers by various distance metrics, this review focuses on the HAD based on machine learning methods, which have witnessed remarkable progress in the recent years. In particular, these studies can generally be grouped into the traditional machine learning and deep-learning-based methods. Several representative HAD methods, including both traditional machine and deep-learning-based methods, are then conducted on four real HSIs in the experiments. Finally, conclusions regarding HAD are summarized, and prospects and future development direction are discussed.},
  keywords = {Anomaly detection,deep learning,Dictionaries,hyperspectral imagery,Hyperspectral imaging,Kernel,Learning systems,machine learning,Manifolds,Task analysis}
}

@article{xu_hyperspectral_2022a,
  title = {Hyperspectral {{Anomaly Detection}} Based on {{Machine Learning}}: {{An Overview}}},
  shorttitle = {Hyperspectral {{Anomaly Detection}} Based on {{Machine Learning}}},
  author = {Xu, Yichu and Zhang, Lefei and Du, Bo and Zhang, Liangpei},
  year = {2022},
  journal = {IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
  publisher = {{IEEE}},
  doi = {10.1109/JSTARS.2022.3167830}
}

@article{yang_diffusion_2022,
  title = {Diffusion {{Models}}: {{A Comprehensive Survey}} of {{Methods}} and {{Applications}}},
  shorttitle = {Diffusion {{Models}}},
  author = {Yang, Ling and Zhang, Zhilong and Song, Yang and Hong, Shenda and Xu, Runsheng and Zhao, Yue and Shao, Yingxia and Zhang, Wentao and Cui, Bin and Yang, Ming-Hsuan},
  year = {2022},
  month = oct,
  number = {arXiv:2209.00796},
  eprint = {2209.00796},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2209.00796},
  url = {http://arxiv.org/abs/2209.00796},
  urldate = {2023-01-25},
  abstract = {Diffusion models have emerged as a powerful new family of deep generative models with record-breaking performance in many applications, including image synthesis, video generation, and molecule design. In this survey, we provide an overview of the rapidly expanding body of work on diffusion models, categorizing the research into three key areas: efficient sampling, improved likelihood estimation, and handling data with special structures. We also discuss the potential for combining diffusion models with other generative models for enhanced results. We further review the wide-ranging applications of diffusion models in fields spanning from computer vision, natural language processing, temporal data modeling, to interdisciplinary applications in other scientific disciplines. This survey aims to provide a contextualized, in-depth look at the state of diffusion models, identifying the key areas of focus and pointing to potential areas for further exploration. Github: https://github.com/YangLing0818/Diffusion-Models-Papers-Survey-Taxonomy.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  journal = {arXiv:2209.00796}
}

@article{yang_loss,
  title = {1 {{Loss}} of Epigenetic Information as a Cause of Mammalian Aging},
  author = {Yang, Jae-Hyun and Hayano, Motoshi and Griffin, Patrick T and Amorim, Joao A and Apostolides, John K and Blanchette, Marco and Munding, Elizabeth M and Bhakta, Mital and Salfati, Elias L and Lu, Yuancheng and Vera, Daniel L and Ross, Jaime M and Coppotelli, Giuseppe and Chew, Ching and Guo, Wei and Yang, Xiaojing and Meer, Margarita V and Tian, Xiao and Dou, Zhixun and Pippin, Jeffrey W and Creswell, Michael L and Mitchell, Sarah J and Das, Abhirup and Thakur, Sachin and Kane, Alice E and Su, Qiao and Mohri, Yasuaki and Nishimura, Emi K and Schaevitz, Laura and Garg, Neha and Balta, Ana-Maria and Rego, Meghan A and Gregory, Meredith and Jakobs, Tatjana C and Zhong, Lei and Wakimoto, Hiroko and Mostoslavsky, Raul and Tsubota, Kazuo and Bonasera, Stephen J and Palmeira, Carlos M and Seidman, Christine E and Wolf, Norman S and Kreiling, Jill A and Sedivy, John M and Murphy, George F and Green, Richard E and Garcia, Benjamin A and Berger, Shelley L and Shankland, Stuart J and Gladyshev, Vadim N and Ksander, Bruce R and Rajman, Luis A and Sinclair, David A},
  language = {en},
  keywords = {❓ Multiple DOI}
}

@article{zhang_multimodal_2023,
  title = {Multimodal {{Chain-of-Thought Reasoning}} in {{Language Models}}},
  author = {Zhang, Zhuosheng and Zhang, Aston and Li, Mu and Zhao, Hai and Karypis, George and Smola, Alex},
  year = {2023},
  journal = {arXiv preprint arXiv:2302.00923},
  eprint = {2302.00923},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  keywords = {⛔ No DOI found}
}

@article{zhang_nerf_2020,
  title = {{{NeRF}}++: {{Analyzing}} and {{Improving Neural Radiance Fields}}},
  shorttitle = {{{NeRF}}++},
  author = {Zhang, Kai and Riegler, Gernot and Snavely, Noah and Koltun, Vladlen},
  year = {2020},
  month = oct,
  number = {arXiv:2010.07492},
  eprint = {2010.07492},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2010.07492},
  urldate = {2023-01-26},
  abstract = {Neural Radiance Fields (NeRF) achieve impressive view synthesis results for a variety of capture settings, including 360 capture of bounded scenes and forward-facing capture of bounded and unbounded scenes. NeRF fits multi-layer perceptrons (MLPs) representing view-invariant opacity and view-dependent color volumes to a set of training images, and samples novel views based on volume rendering techniques. In this technical report, we first remark on radiance fields and their potential ambiguities, namely the shape-radiance ambiguity, and analyze NeRF's success in avoiding such ambiguities. Second, we address a parametrization issue involved in applying NeRF to 360 captures of objects within large-scale, unbounded 3D scenes. Our method improves view synthesis fidelity in this challenging scenario. Code is available at https://github.com/Kai-46/nerfplusplus.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  journal = {arXiv:2010.07492}
}

@article{zhang_opt_2022,
  title = {{{OPT}}: {{Open Pre-trained Transformer Language Models}}},
  shorttitle = {{{OPT}}},
  author = {Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and Mihaylov, Todor and Ott, Myle and Shleifer, Sam and Shuster, Kurt and Simig, Daniel and Koura, Punit Singh and Sridhar, Anjali and Wang, Tianlu and Zettlemoyer, Luke},
  year = {2022},
  month = jun,
  number = {arXiv:2205.01068},
  eprint = {2205.01068},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2205.01068},
  urldate = {2023-02-01},
  abstract = {Large language models, which are often trained for hundreds of thousands of compute days, have shown remarkable capabilities for zero- and few-shot learning. Given their computational cost, these models are difficult to replicate without significant capital. For the few that are available through APIs, no access is granted to the full model weights, making them difficult to study. We present Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers. We show that OPT-175B is comparable to GPT-3, while requiring only 1/7th the carbon footprint to develop. We are also releasing our logbook detailing the infrastructure challenges we faced, along with code for experimenting with all of the released models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  journal = {arXiv:2205.01068}
}

@inproceedings{zhao_finetuning_2021b,
  title = {Fine-{{Tuning BERT Model}} for {{Materials Named Entity Recognition}}},
  booktitle = {2021 {{IEEE International Conference}} on {{Big Data}} ({{Big Data}})},
  author = {Zhao, Xintong and Greenberg, Jane and An, Yuan and Hu, Xiaohua Tony},
  year = {2021},
  month = dec,
  pages = {3717--3720},
  doi = {10.1109/BigData52589.2021.9671697},
  abstract = {Scientific literature presents a wellspring of cutting-edge knowledge for materials science, including valuable data (e.g., numerical data from experiment results, material properties and structure). These data are critical for accelerating materials discovery by data-driven machine learning (ML) methods. The challenge is, it is impossible for humans to manually extract and retain this knowledge due to the extensive and growing volume of publications.To this end, we explore a fine-tuned BERT model for extracting knowledge. Our preliminary results show that our fine-tuned Bert model reaches an f-score of 85\% for the materials named entity recognition task. The paper covers background, related work, methodology including tuning parameters, and our overall performance evaluation. Our discussion offers insights into our results, and points to directions for next steps.},
  keywords = {Adaptation models,Analytical models,BERT,Big Data,Bit error rate,materials science,named entity recognition,natural language processing,Solid modeling,Text recognition,Transformers}
}

@article{zheng_fast_2022,
  title = {Fast {{Sampling}} of {{Diffusion Models}} via {{Operator Learning}}},
  author = {Zheng, Hongkai and Nie, Weili and Vahdat, Arash and Azizzadenesheli, Kamyar and Anandkumar, Anima},
  year = {2022},
  month = nov,
  number = {arXiv:2211.13449},
  eprint = {2211.13449},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2211.13449},
  url = {http://arxiv.org/abs/2211.13449},
  urldate = {2023-01-18},
  abstract = {Diffusion models have found widespread adoption in various areas. However, sampling from them is slow because it involves emulating a reverse process with hundreds-to-thousands of network evaluations. Inspired by the success of neural operators in accelerating differential equations solving, we approach this problem by solving the underlying neural differential equation from an operator learning perspective. We examine probability flow ODE trajectories in diffusion models and observe a compact energy spectrum that can be learned efficiently in Fourier space. With this insight, we propose diffusion Fourier neural operator (DFNO) with temporal convolution in Fourier space to parameterize the operator that maps initial condition to the solution trajectory, which is a continuous function in time. DFNO can be applied to any diffusion model and generate high-quality samples in one model forward call. Our method achieves the state-of-the-art FID of 4.72 on CIFAR-10 using only one model evaluation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  journal = {arXiv:2211.13449}
}

@article{zhong_neural_2022,
  title = {A {{Neural ODE Interpretation}} of {{Transformer Layers}}},
  author = {Zhong, Yaofeng Desmond and Zhang, Tongtao and Chakraborty, Amit and Dey, Biswadip},
  year = {2022},
  month = dec,
  number = {arXiv:2212.06011},
  eprint = {2212.06011},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2212.06011},
  url = {http://arxiv.org/abs/2212.06011},
  urldate = {2023-01-18},
  abstract = {Transformer layers, which use an alternating pattern of multi-head attention and multi-layer perceptron (MLP) layers, provide an effective tool for a variety of machine learning problems. As the transformer layers use residual connections to avoid the problem of vanishing gradients, they can be viewed as the numerical integration of a differential equation. In this extended abstract, we build upon this connection and propose a modification of the internal architecture of a transformer layer. The proposed model places the multi-head attention sublayer and the MLP sublayer parallel to each other. Our experiments show that this simple modification improves the performance of transformer networks in multiple tasks. Moreover, for the image classification task, we show that using neural ODE solvers with a sophisticated integration scheme further improves performance.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  journal = {arXiv:2212.06011}
}

@article{zhou_introduction_2012,
  title = {Introduction to {{Metal}}–{{Organic Frameworks}}},
  author = {Zhou, Hong-Cai and Long, Jeffrey R. and Yaghi, Omar M.},
  year = {2012},
  month = feb,
  journal = {Chemical Reviews},
  volume = {112},
  number = {2},
  pages = {673--674},
  issn = {0009-2665, 1520-6890},
  doi = {10.1021/cr300014x},
  url = {https://pubs.acs.org/doi/10.1021/cr300014x},
  urldate = {2023-02-16},
  language = {en}
}

@article{zhou_large_2022,
  title = {Large {{Language Models Are Human-Level Prompt Engineers}}},
  author = {Zhou, Yongchao and Muresanu, Andrei Ioan and Han, Ziwen and Paster, Keiran and Pitis, Silviu and Chan, Harris and Ba, Jimmy},
  year = {2022},
  month = nov,
  number = {arXiv:2211.01910},
  eprint = {2211.01910},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2211.01910},
  url = {http://arxiv.org/abs/2211.01910},
  urldate = {2023-02-16},
  abstract = {By conditioning on natural language instructions, large language models (LLMs) have displayed impressive capabilities as general-purpose computers. However, task performance depends significantly on the quality of the prompt used to steer the model, and most effective prompts have been handcrafted by humans. Inspired by classical program synthesis and the human approach to prompt engineering, we propose Automatic Prompt Engineer (APE) for automatic instruction generation and selection. In our method, we treat the instruction as the "program," optimized by searching over a pool of instruction candidates proposed by an LLM in order to maximize a chosen score function. To evaluate the quality of the selected instruction, we evaluate the zero-shot performance of another LLM following the selected instruction. Experiments on 24 NLP tasks show that our automatically generated instructions outperform the prior LLM baseline by a large margin and achieve better or comparable performance to the instructions generated by human annotators on 19/24 tasks. We conduct extensive qualitative and quantitative analyses to explore the performance of APE. We show that APE-engineered prompts can be applied to steer models toward truthfulness and/or informativeness, as well as to improve few-shot learning performance by simply prepending them to standard in-context learning prompts. Please check out our webpage at https://sites.google.com/view/automatic-prompt-engineer.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  journal = {arXiv:2211.01910}
}
