\documentclass[a4paper,12pt]{article}

\usepackage[margin=1in]{geometry} % full-width

% AMS Packages
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}

% Unicode
\usepackage[utf8]{inputenc}
\usepackage{hyperref}
\hypersetup{
    unicode,
    colorlinks,
%   breaklinks,
%   urlcolor=cyan,
    linkcolor=blue,
    pdfauthor={Author One, Author Two, Author Three},
    pdftitle={A simple article template},
    pdfsubject={A simple article template},
    pdfkeywords={article, template, simple},
    pdfproducer={LaTeX},
    pdfcreator={pdflatex}
}

% Vietnamese
%\usepackage{vntex}

% Natbib
\usepackage[sort&compress,numbers,square]{natbib}
\bibliographystyle{ieeetr}

% Theorem, Lemma, etc
% \theoremstyle{plain}
% \newtheorem{theorem}{Theorem}
% \newtheorem{corollary}[theorem]{Corollary}
% \newtheorem{lemma}[theorem]{Lemma}
% \newtheorem{claim}{Claim}[theorem]
% \newtheorem{axiom}[theorem]{Axiom}
% \newtheorem{conjecture}[theorem]{Conjecture}
% \newtheorem{fact}[theorem]{Fact}
% \newtheorem{hypothesis}[theorem]{Hypothesis}
% \newtheorem{assumption}[theorem]{Assumption}
% \newtheorem{proposition}[theorem]{Proposition}
% \newtheorem{criterion}[theorem]{Criterion}
% \theoremstyle{definition}
% \newtheorem{definition}[theorem]{Definition}
% \newtheorem{example}[theorem]{Example}
% \newtheorem{remark}[theorem]{Remark}
% \newtheorem{problem}[theorem]{Problem}
% \newtheorem{principle}[theorem]{Principle}

\usepackage{graphicx, color}
\graphicspath{{figures/}}

%\usepackage[linesnumbered,ruled,vlined,commentsnumbered]{algorithm2e} % use algorithm2e for typesetting algorithms
\usepackage{algorithm, algpseudocode} % use algorithm and algorithmicx for typesetting algorithms
\usepackage{mathrsfs} % for \mathscr command

\usepackage{lipsum}

% Author info
\title{Expos√© on Using Large Language Models for Automated Data Extraction from Scientific Literature}
\author{Felix Karg\footnote{This work has partially been augmented using ChatGPT}}

\date{\today}
% \renewcommand{\abstractname}{\vspace{-\baselineskip}} % clear 'abstract' from abstract

\newcommand{\margtodo}                                 % used by \todo command
{\marginpar{\textbf{\textcolor{red}{ToDo}}}{}}
\newcommand{\todo}[1]
{{\textbf{\textcolor{red}{[\margtodo{}#1]}}}{}}   % for todo-notes inside 


\begin{document}
\maketitle

% \begin{abstract}

    % \noindent\textbf{Keywords:} article, template, simple
% \end{abstract}

% \tableofcontents

\section{Introduction}
A large amount of scientific knowledge is scattered across millions of research
papers. Often, this research is not in standardized machine-readable formats,
which makes it difficult or impossible to build on prior work using powerful
tools to extract further knowledge.  % \todo{expand on LLMs and limits here?}
% setting up and running llm individually is expensive in time and compute

\section{Motivation}
Take for example the field of synthesizing Metal-Organic Frameworks (MOFs)
\cite{zhou_introduction_2012}. While numerous detailed descriptions of
synthesis procedures exist, they are not in a machine-readable format, which
prevents effective application of state-of-the-art techniques such as automated
experimentation \cite{shi_automated_2021} or synthesis prediction
\cite{luo_mof_2022}. Thus, we intend to create a pipeline for deriving
machine-readable information on MOF synthesis parameters from given questions
on provided scientific articles.


\section{Background}
\paragraph{Rule-Based Entity Recognition}
There have long been rule-based approaches for the recognition of individual
entities. ChemTagger \cite{hawizy_chemicaltagger_2011} clearly demonstrated
that simple rule-based systems can sometimes extract much of the requested
information. While they often achieve high precision for simple tasks,
they fail in answering more complex queries, such as the relation between
two entities.

\paragraph{Language Models}
With 'Attention is All you Need' \cite{vaswani_attention_2017}, Google
introduced the transformer architecture for language models and demonstrated
significant improvements. Soon, BERT \cite{devlin_bert_2018} followed, a model
which is conceptually simple and empirically powerful. It was soon demonstrated
that BERT can be easily fine-tuned for named entity recognition in materials
science \cite{zhao_finetuning_2021}. OpenAI pushed scaling forward with their
GPT2 \cite{radford_language_2019} model, which was substantially larger than
BERT. Step-by-step, these models enabled more sophisticated extraction requests.

\paragraph{Large Language Models}
With the introduction of GPT3 \cite{brown_language_2020} OpenAI trailblazed the
era of Large Language Models. This model enabled more sophisticated information
extraction requests with little fine-tuning \cite{dunn_structured_2022}. Soon,
open-source variants such as OPT \cite{zhang_opt_2022} followed. It was also
demonstrated with Chinchilla \cite{hoffmann_training_2022} and CoTR
\cite{zhang_multimodal_2023}, that these large language models are
substantially overparametrized and undertrained.

% \paragraph{Downstream Usages}
% MOF Synthesis Prediction Enabled by Automatic Data Mining and Machine Learning \cite{luo_mof_2022}
% Doing Synthesis prediction based on an automatic data extraction pipeline
% \todo{rewrite}

\section{Scientific Questions}
Use Large Language Models to demonstrate automated extraction of unstructured
text from scientific literature for the creation of a database with otherwise
unavailable information on MOF synthesis. By doing so, we create a pipeline
that can easily be adapted to numerous other data extraction tasks.

Specifically, using OPT \cite{zhang_opt_2022} empirically test if accuracy can
be improved via 1) fine-tuning and 2) prompt engineering. Additionally,
test if 3) model size can be reduced by using distillation
\cite{sun_patient_2019}, and how it will affect accuracy as well as compute and
memory requirements. Distillation would enable considerable model parameter
reduction with little loss in accuracy, which could make it substantially
less compute intensive to fine-tune and run.

% Benchmarking accuracy of data extraction from scientific literature using state-of-the-art LLMs and other tools: how well do various existing methods do, and (how much) can priming, fine-tuning and prompt engineering improve this. \todo{rewrite}
% 
% Specifically, compare the accuracy of:
% \begin{itemize}
%     \item what the model already knows: here, the article would not be in context
%     \item what it can easily extract from the article: when provided in
%         context, how well it can answer questions relating the content.
%     \item particarly, without much prompt engineering or fine-tuning the model
%     \item with prompt engineering: attempt to increase accuracy by finding good prompts for that
%     \item after task-based fine-tuning (without context, in-context, and with or without good prompts)
%     \begin{itemize}
%         \item articles included in fine-tuning
%         \item articles not fine-tuned on
%         \item experiment with fine-tuning approaches
%     \end{itemize}
%     \item Models of varying sizes
%     \begin{itemize}
%         \item the varying sizes of available OPT-models \cite{zhang_opt_2022}
%         \item given enough time, try to use distillation \cite{sun_patient_2019} to compress the model parameter size
%     \end{itemize}
% \end{itemize}

% \section{Objectives}
% Objectives of the work (both mandatory objectives and optional additional objectives)

% \section{Intermediate Steps}
% Concrete work steps
% \begin{enumerate}
%     \item Take exemplary / arbitrary synthesis paper
%     \item get paragraph classification to work
%     \item figure out how to pass the relevant paragraphs as LLM context
%     \item determine accuracy of answering the given questions based on reference database
%     \item first without, later with prompt engineering and fine-tuning of the model
% \end{enumerate}
% \todo{combine with schedule}

\section{Schedule}
\begin{figure}[ht!]
    \includegraphics[width=\textwidth]{img/thesis_gantt}
    \caption{Exemplary timeline. Details are subject to change.}
\end{figure}


\subsection{Literature Research}
In this step, we will conduct a thorough review of the relevant literature
related to our topic. This includes reading academic articles, research papers,
and other publications to gain a better understanding of the current state of
knowledge in the field. By continuously reading relevant literature, we can
stay up-to-date with the latest advancements and identify gaps in the research
that we can address in our own work.


\subsection{Train Simple GPT}
We will train a small GPT model to gain a deeper understanding of its
architecture, training procedure, and properties. By doing this, we can get
hands-on experience with the model and better understand its strengths and
limitations. This will help us to make informed decisions about how to
fine-tune the model for our specific task.


\subsection{Set up OPT models on Cluster}
To set up the open-source OPT models on a cluster, we will need to carefully
configure the models to ensure that our experiments are run on a reliable and
scalable computing infrastructure. While the models can be downloaded, the
configuration process may not be straightforward, and it will be critical to
get it right to ensure the reliability and scalability of our experiments.


\subsection{Fine-tune model}
In this step, we will construct and train the model on select examples, with
intermediate annotations or validation in-between, similar to what is described
in \cite{dunn_structured_2022}. The first goal is to increase the task success
rate, which means answering questions in the requested machine-readable format.
The second goal is to increase accuracy. To achieve high accuracy, we may
manually annotate a few examples, and augment using partially annotated ones.


\subsection{Prompt Engineering}
In this step, we will apply deep introspection and automatic prompt engineering
\cite{zhou_large_2022} to increase the accuracy of generated databases. Prompt
engineering involves designing the prompts that the model will use to generate
responses. By optimizing the prompts, we can improve the quality of the model's
output and make it more accurate.


\subsection{Distillation}
Distillation is a technique for reducing the size of a large model while
maintaining its accuracy. In this step, we will apply distillation
\cite{sun_patient_2019} to our model to reduce its parameter size while keeping
accuracy high. This will make the model more efficient and easier to deploy in
production.


\subsection{Run Experiments}
In this step, we will run detailed experiments to evaluate the performance of
our model. We will generate graphs, tables, and databases of extracted
information to analyze and interpret the results. By running experiments, we
can validate the effectiveness of our approach and identify areas for
improvement.


\subsection{Write Thesis}
In the final step, we will write an extensive scientific article as the
concluding work of our master's degree. This will involve summarizing our
research, detailing our methodology, presenting our results, and discussing the
implications of our findings. Writing the thesis is an essential part of the
research process and allows us to share our insights and contributions with the
broader academic community.


\bibliography{references.bib}
 
\end{document}
