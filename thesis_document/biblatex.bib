@misc{abadiTensorFlowLargeScaleMachine2015,
  title      = {{{TensorFlow}}: {{Large-Scale Machine Learning}} on {{Heterogeneous Distributed Systems}}},
  shorttitle = {{{TensorFlow}}},
  author     = {Abadi, Martín and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Goodfellow, Ian and Harp, Andrew and Irving, Geoffrey and Isard, Michael and Jia, Yangqing and Jozefowicz, Rafal and Kaiser, Lukasz and Kudlur, Manjunath and Levenberg, Josh and Mané, Dan and Monga, Rajat and Moore, Sherry and Murray, Derek and Olah, Chris and Schuster, Mike and Shlens, Jonathon and Steiner, Benoit and Sutskever, Ilya and Talwar, Kunal and Tucker, Paul and Vanhoucke, Vincent and Vasudevan, Vijay and Viégas, Fernanda and Vinyals, Oriol and Warden, Pete and Wattenberg, Martin and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
  date       = {2015},
  url        = {https://www.tensorflow.org/},
  file       = {/home/henrik/Zotero/storage/9UNQG7BG/Abadi et al. - 2015 - TensorFlow Large-Scale Machine Learning on Hetero.pdf}
}

@misc{hanebeck_flux_2018,
	title = {{FLUX}: Progressive State Estimation Based on Zakai-type Distributed Ordinary Differential Equations},
	url = {http://arxiv.org/abs/1808.02825},
	shorttitle = {{FLUX}},
	abstract = {We propose a homotopy continuation method called {FLUX} for approximating complicated probability density functions. It is based on progressive processing for smoothly morphing a given density into the desired one. Distributed ordinary diﬀerential equations ({DODEs}) with an artiﬁcial time γ ∈ [0, 1] are derived for describing the evolution from the initial density to the desired ﬁnal density. For a ﬁnite-dimensional parametrization, the {DODEs} are converted to a system of ordinary diﬀerential equations ({SODEs}), which are solved for γ ∈ [0, 1] and return the desired result for γ = 1. This includes parametric representations such as Gaussians or Gaussian mixtures and nonparametric setups such as sample sets. In the latter case, we obtain a particle ﬂow between the two densities along the artiﬁcial time.},
	number = {{arXiv}:1808.02825},
	publisher = {{arXiv}},
	author = {Hanebeck, Uwe D.},
	urldate = {2023-01-18},
	date = {2018-08-08},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1808.02825 [cs]},
	keywords = {Electrical Engineering and Systems Science - Systems and Control},
	annotation = {Comment: 19 pages},
	file = {Hanebeck - 2018 - FLUX Progressive State Estimation Based on Zakai-.pdf:/home/pars/Coding/papers/Zotero/storage/CF8SZCJQ/Hanebeck - 2018 - FLUX Progressive State Estimation Based on Zakai-.pdf:application/pdf},
}

@inproceedings{prossel_dirac_2022,
	location = {Linköping, Sweden},
	title = {Dirac Mixture Reduction Using Wasserstein Distances on Projected Cumulative Distributions},
	isbn = {978-1-73774-972-1},
	url = {https://ieeexplore.ieee.org/document/9841286/},
	doi = {10.23919/FUSION49751.2022.9841286},
	abstract = {The reapproximation of discrete probability densities is a common task in sample-based filters such as the particle filter. It can be viewed as the approximation of a given Dirac mixture density with another one, typically with fewer samples. In this paper, the Wasserstein distance is established as a suitable measure to compare two Dirac mixtures. The resulting minimization problem is also known as location-allocation or facility location problem and cannot be solved in polynomial time. Therefore, the well-known sliced Wasserstein distance is introduced as a replacement and its ties to the projected cumulative distribution ({PCD}) are shown. An iterative algorithm is proposed to minimize the sliced Wasserstein distance between the given distribution and approximation.},
	eventtitle = {2022 25th International Conference on Information Fusion ({FUSION})},
	pages = {1--8},
	booktitle = {2022 25th International Conference on Information Fusion ({FUSION})},
	publisher = {{IEEE}},
	author = {Prossel, Dominik and Hanebeck, Uwe D.},
	urldate = {2023-01-18},
	date = {2022-07-04},
	langid = {english},
	file = {Prossel and Hanebeck - 2022 - Dirac Mixture Reduction Using Wasserstein Distance.pdf:/home/pars/Coding/papers/Zotero/storage/YSB7GSIM/Prossel and Hanebeck - 2022 - Dirac Mixture Reduction Using Wasserstein Distance.pdf:application/pdf},
}

@book{agarwal_introduction_2008,
	location = {New York, {NY}},
	title = {An Introduction to Ordinary Differential Equations},
	isbn = {978-0-387-71275-8 978-0-387-71276-5},
	url = {http://link.springer.com/10.1007/978-0-387-71276-5},
	publisher = {Springer New York},
	author = {Agarwal, Ravi P. and O'Regan, Donal},
	urldate = {2023-01-18},
	date = {2008},
	langid = {english},
	doi = {10.1007/978-0-387-71276-5},
	file = {Agarwal and O'Regan - 2008 - An Introduction to Ordinary Differential Equations.pdf:/home/pars/Coding/papers/Zotero/storage/IRF8XZB5/Agarwal and O'Regan - 2008 - An Introduction to Ordinary Differential Equations.pdf:application/pdf},
}

@misc{lu_understanding_2019,
	title = {Understanding and Improving Transformer From a Multi-Particle Dynamic System Point of View},
	url = {http://arxiv.org/abs/1906.02762},
	abstract = {The Transformer architecture is widely used in natural language processing. Despite its success, the design principle of the Transformer remains elusive. In this paper, we provide a novel perspective towards understanding the architecture: we show that the Transformer can be mathematically interpreted as a numerical Ordinary Differential Equation ({ODE}) solver for a convection-diffusion equation in a multi-particle dynamic system. In particular, how words in a sentence are abstracted into contexts by passing through the layers of the Transformer can be interpreted as approximating multiple particles' movement in the space using the Lie-Trotter splitting scheme and the Euler's method. Given this {ODE}'s perspective, the rich literature of numerical analysis can be brought to guide us in designing effective structures beyond the Transformer. As an example, we propose to replace the Lie-Trotter splitting scheme by the Strang-Marchuk splitting scheme, a scheme that is more commonly used and with much lower local truncation errors. The Strang-Marchuk splitting scheme suggests that the self-attention and position-wise feed-forward network ({FFN}) sub-layers should not be treated equally. Instead, in each layer, two position-wise {FFN} sub-layers should be used, and the self-attention sub-layer is placed in between. This leads to a brand new architecture. Such an {FFN}-attention-{FFN} layer is "Macaron-like", and thus we call the network with this new architecture the Macaron Net. Through extensive experiments, we show that the Macaron Net is superior to the Transformer on both supervised and unsupervised learning tasks. The reproducible codes and pretrained models can be found at https://github.com/zhuohan123/macaron-net},
	number = {{arXiv}:1906.02762},
	publisher = {{arXiv}},
	author = {Lu, Yiping and Li, Zhuohan and He, Di and Sun, Zhiqing and Dong, Bin and Qin, Tao and Wang, Liwei and Liu, Tie-Yan},
	urldate = {2023-01-18},
	date = {2019-06-06},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1906.02762 [cs, stat]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Lu et al. - 2019 - Understanding and Improving Transformer From a Mul.pdf:/home/pars/Coding/papers/Zotero/storage/PE98L7I9/Lu et al. - 2019 - Understanding and Improving Transformer From a Mul.pdf:application/pdf},
}

@misc{chen_neural_2019,
	title = {Neural Ordinary Differential Equations},
	url = {http://arxiv.org/abs/1806.07366},
	abstract = {We introduce a new family of deep neural network models. Instead of specifying a discrete sequence of hidden layers, we parameterize the derivative of the hidden state using a neural network. The output of the network is computed using a blackbox differential equation solver. These continuous-depth models have constant memory cost, adapt their evaluation strategy to each input, and can explicitly trade numerical precision for speed. We demonstrate these properties in continuous-depth residual networks and continuous-time latent variable models. We also construct continuous normalizing ﬂows, a generative model that can train by maximum likelihood, without partitioning or ordering the data dimensions. For training, we show how to scalably backpropagate through any {ODE} solver, without access to its internal operations. This allows end-to-end training of {ODEs} within larger models.},
	number = {{arXiv}:1806.07366},
	publisher = {{arXiv}},
	author = {Chen, Ricky T. Q. and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David},
	urldate = {2023-01-18},
	date = {2019-12-13},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1806.07366 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {Chen et al. - 2019 - Neural Ordinary Differential Equations.pdf:/home/pars/Coding/papers/Zotero/storage/PQQCUS9R/Chen et al. - 2019 - Neural Ordinary Differential Equations.pdf:application/pdf},
}

@article{ruoff_progressive_nodate,
	title = {Progressive correction for deterministic Dirac mixture approximations},
	abstract = {Since the advent of Monte-Carlo particle ﬁltering, particle representations of densities have become increasingly popular due to their ﬂexibility and implicit adaptive resolution. In this paper, an algorithm for the multiplication of a systematic Dirac mixture ({DM}) approximation with a continuous likelihood function is presented, which applies a progressive correction scheme, in order to avoid the particle degeneration problem. The preservation of sample regularity and therefore, representation quality of the underlying smooth density, is ensured by including a new measure of smoothness for Dirac mixtures, the {DM} energy, into the distance measure. A comparison to common correction schemes in Monte-Carlo methods reveals large improvements especially in cases of small overlap between the likelihood and prior density, as well as for multi-modal likelihoods.},
	author = {Ruoff, Patrick and Krauthausen, Peter and Hanebeck, Uwe D},
	langid = {english},
	keywords = {⛔ No {DOI} found},
	file = {Ruoff et al. - Progressive correction for deterministic Dirac mix.pdf:/home/pars/Coding/papers/Zotero/storage/4UYV2QTP/Ruoff et al. - Progressive correction for deterministic Dirac mix.pdf:application/pdf},
}

@article{choromanski_ode_2020,
	title = {Ode to an {ODE}},
	volume = {33},
	pages = {3338--3350},
	journaltitle = {Advances in Neural Information Processing Systems},
	author = {Choromanski, Krzysztof M. and Davis, Jared Quincy and Likhosherstov, Valerii and Song, Xingyou and Slotine, Jean-Jacques and Varley, Jacob and Lee, Honglak and Weller, Adrian and Sindhwani, Vikas},
	date = {2020},
	keywords = {⛔ No {DOI} found},
	file = {Full Text:/home/pars/Coding/papers/Zotero/storage/UXJB3QW8/Choromanski et al. - 2020 - Ode to an ODE.pdf:application/pdf;Snapshot:/home/pars/Coding/papers/Zotero/storage/SFXF8HHM/228669109aa3ab1b4ec06b7722efb105-Abstract.html:text/html},
}

@article{liu_second-order_2021,
	title = {Second-order neural {ODE} optimizer},
	volume = {34},
	pages = {25267--25279},
	journaltitle = {Advances in Neural Information Processing Systems},
	author = {Liu, Guan-Horng and Chen, Tianrong and Theodorou, Evangelos},
	date = {2021},
	keywords = {⛔ No {DOI} found},
	file = {Full Text:/home/pars/Coding/papers/Zotero/storage/ZE2D6TGJ/Liu et al. - 2021 - Second-order neural ODE optimizer.pdf:application/pdf;Snapshot:/home/pars/Coding/papers/Zotero/storage/R9TH64PI/d4c2e4a3297fe25a71d030b67eb83bfc-Abstract.html:text/html},
}

@article{bilos_neural_2021,
	title = {Neural Flows: Efficient Alternative to Neural {ODEs}},
	volume = {34},
	shorttitle = {Neural Flows},
	pages = {21325--21337},
	journaltitle = {Advances in Neural Information Processing Systems},
	author = {Biloš, Marin and Sommer, Johanna and Rangapuram, Syama Sundar and Januschowski, Tim and Günnemann, Stephan},
	date = {2021},
	keywords = {⛔ No {DOI} found},
	file = {Full Text:/home/pars/Coding/papers/Zotero/storage/DMETNCMU/Biloš et al. - 2021 - Neural Flows Efficient Alternative to Neural ODEs.pdf:application/pdf;Snapshot:/home/pars/Coding/papers/Zotero/storage/CWYKI9D4/b21f9f98829dea9a48fd8aaddc1f159d-Abstract.html:text/html},
}

@article{finlay_how_2020,
	title = {How to train your neural ode},
	journaltitle = {{arXiv} preprint {arXiv}:2002.02798},
	author = {Finlay, Chris and Jacobsen, Jörn-Henrik and Nurbekyan, Levon and Oberman, Adam M.},
	date = {2020},
	keywords = {⛔ No {DOI} found},
	file = {Full Text:/home/pars/Coding/papers/Zotero/storage/W88YSN49/Finlay et al. - 2020 - How to train your neural ode.pdf:application/pdf},
}

@article{ott_when_2020,
	title = {When are neural {ODE} solutions proper {ODEs}?},
	journaltitle = {{arXiv} preprint {arXiv}:2007.15386},
	author = {Ott, Katharina and Katiyar, Prateek and Hennig, Philipp and Tiemann, Michael},
	date = {2020},
	keywords = {⛔ No {DOI} found},
	file = {Full Text:/home/pars/Coding/papers/Zotero/storage/RTKZYJB8/Ott et al. - 2020 - When are neural ODE solutions proper ODEs.pdf:application/pdf;Snapshot:/home/pars/Coding/papers/Zotero/storage/IK6AHRMC/2007.html:text/html},
}

@inproceedings{chang_reversible_2018,
	title = {Reversible architectures for arbitrarily deep residual neural networks},
	volume = {32},
	doi = {10.1609/aaai.v32i1.11668},
	booktitle = {Proceedings of the {AAAI} conference on artificial intelligence},
	author = {Chang, Bo and Meng, Lili and Haber, Eldad and Ruthotto, Lars and Begert, David and Holtham, Elliot},
	date = {2018},
	note = {Issue: 1},
	file = {Full Text:/home/pars/Coding/papers/Zotero/storage/B3FVXL6Y/Chang et al. - 2018 - Reversible architectures for arbitrarily deep resi.pdf:application/pdf;Snapshot:/home/pars/Coding/papers/Zotero/storage/UQPIQ4DD/11668.html:text/html},
}

@article{dupont_augmented_2019,
	title = {Augmented neural odes},
	volume = {32},
	journaltitle = {Advances in Neural Information Processing Systems},
	author = {Dupont, Emilien and Doucet, Arnaud and Teh, Yee Whye},
	date = {2019},
	keywords = {⛔ No {DOI} found},
	file = {Full Text:/home/pars/Coding/papers/Zotero/storage/QRTAWQBU/Dupont et al. - 2019 - Augmented neural odes.pdf:application/pdf;Snapshot:/home/pars/Coding/papers/Zotero/storage/NEEBUPHQ/21be9a4bd4f81549a9d1d241981cec3c-Abstract.html:text/html},
}

@article{eliasof_pde-gcn_2021,
	title = {Pde-gcn: Novel architectures for graph neural networks motivated by partial differential equations},
	volume = {34},
	shorttitle = {Pde-gcn},
	pages = {3836--3849},
	journaltitle = {Advances in Neural Information Processing Systems},
	author = {Eliasof, Moshe and Haber, Eldad and Treister, Eran},
	date = {2021},
	keywords = {⛔ No {DOI} found},
	file = {Full Text:/home/pars/Coding/papers/Zotero/storage/99U5UYIJ/Eliasof et al. - 2021 - Pde-gcn Novel architectures for graph neural netw.pdf:application/pdf;Snapshot:/home/pars/Coding/papers/Zotero/storage/LWPTC86L/1f9f9d8ff75205aa73ec83e543d8b571-Abstract.html:text/html},
}

@article{qiu_accuracy_2021,
	title = {Accuracy and Architecture Studies of Residual Neural Network solving Ordinary Differential Equations},
	journaltitle = {{arXiv} preprint {arXiv}:2101.03583},
	author = {Qiu, Changxin and Bendickson, Aaron and Kalyanapu, Joshua and Yan, Jue},
	date = {2021},
	keywords = {⛔ No {DOI} found},
	file = {Full Text:/home/pars/Coding/papers/Zotero/storage/NJZ34SBG/Qiu et al. - 2021 - Accuracy and Architecture Studies of Residual Neur.pdf:application/pdf;Snapshot:/home/pars/Coding/papers/Zotero/storage/PSTS4SWB/2101.html:text/html},
}

@article{ruthotto_deep_2020,
	title = {Deep neural networks motivated by partial differential equations},
	volume = {62},
	doi = {10.1007/s10851-019-00903-1},
	pages = {352--364},
	number = {3},
	journaltitle = {Journal of Mathematical Imaging and Vision},
	author = {Ruthotto, Lars and Haber, Eldad},
	date = {2020},
	note = {Publisher: Springer},
	file = {Full Text:/home/pars/Coding/papers/Zotero/storage/DDPW6FYX/Ruthotto and Haber - 2020 - Deep neural networks motivated by partial differen.pdf:application/pdf;Snapshot:/home/pars/Coding/papers/Zotero/storage/JZKBNR3Z/s10851-019-00903-1.html:text/html},
}

@article{haber_stable_2017,
	title = {Stable architectures for deep neural networks},
	volume = {34},
	doi = {10.1088/1361-6420/aa9a90},
	pages = {014004},
	number = {1},
	journaltitle = {Inverse problems},
	author = {Haber, Eldad and Ruthotto, Lars},
	date = {2017},
	note = {Publisher: {IOP} Publishing},
	annotation = {Add regularization to learn representations that generalize better (even with less data), only skimmed so far
},
	file = {Full Text:/home/pars/Coding/papers/Zotero/storage/5QMT7HLV/Haber and Ruthotto - 2017 - Stable architectures for deep neural networks.pdf:application/pdf;Snapshot:/home/pars/Coding/papers/Zotero/storage/FSK72R4K/meta.html:text/html},
}

@article{vaswani_attention_2017,
	title = {Attention is all you need},
	volume = {30},
	journaltitle = {Advances in neural information processing systems},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	date = {2017},
	keywords = {⛔ No {DOI} found},
	file = {Full Text:/home/pars/Coding/papers/Zotero/storage/AKIKFN9Y/Vaswani et al. - 2017 - Attention is all you need.pdf:application/pdf;Snapshot:/home/pars/Coding/papers/Zotero/storage/DFZGDK5N/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html:text/html},
}

@misc{tian_mononerf_2022,
	title = {{MonoNeRF}: Learning a Generalizable Dynamic Radiance Field from Monocular Videos},
	url = {http://arxiv.org/abs/2212.13056},
	doi = {10.48550/arXiv.2212.13056},
	shorttitle = {{MonoNeRF}},
	abstract = {In this paper, we target at the problem of learning a generalizable dynamic radiance field from monocular videos. Different from most existing {NeRF} methods that are based on multiple views, monocular videos only contain one view at each timestamp, thereby suffering from ambiguity along the view direction in estimating point features and scene flows. Previous studies such as {DynNeRF} disambiguate point features by positional encoding, which is not transferable and severely limits the generalization ability. As a result, these methods have to train one independent model for each scene and suffer from heavy computational costs when applying to increasing monocular videos in real-world applications. To address this, We propose {MonoNeRF} to simultaneously learn point features and scene flows with point trajectory and feature correspondence constraints across frames. More specifically, we learn an implicit velocity field to estimate point trajectory from temporal features with Neural {ODE}, which is followed by a flow-based feature aggregation module to obtain spatial features along the point trajectory. We jointly optimize temporal and spatial features by training the network in an end-to-end manner. Experiments show that our {MonoNeRF} is able to learn from multiple scenes and support new applications such as scene editing, unseen frame synthesis, and fast novel scene adaptation.},
	number = {{arXiv}:2212.13056},
	publisher = {{arXiv}},
	author = {Tian, Fengrui and Du, Shaoyi and Duan, Yueqi},
	urldate = {2023-01-18},
	date = {2022-12-26},
	eprinttype = {arxiv},
	eprint = {2212.13056 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/pars/Coding/papers/Zotero/storage/77J6KKKL/Tian et al. - 2022 - MonoNeRF Learning a Generalizable Dynamic Radianc.pdf:application/pdf;arXiv.org Snapshot:/home/pars/Coding/papers/Zotero/storage/X2DRR535/2212.html:text/html},
}

@misc{zhong_neural_2022,
	title = {A Neural {ODE} Interpretation of Transformer Layers},
	url = {http://arxiv.org/abs/2212.06011},
	doi = {10.48550/arXiv.2212.06011},
	abstract = {Transformer layers, which use an alternating pattern of multi-head attention and multi-layer perceptron ({MLP}) layers, provide an effective tool for a variety of machine learning problems. As the transformer layers use residual connections to avoid the problem of vanishing gradients, they can be viewed as the numerical integration of a differential equation. In this extended abstract, we build upon this connection and propose a modification of the internal architecture of a transformer layer. The proposed model places the multi-head attention sublayer and the {MLP} sublayer parallel to each other. Our experiments show that this simple modification improves the performance of transformer networks in multiple tasks. Moreover, for the image classification task, we show that using neural {ODE} solvers with a sophisticated integration scheme further improves performance.},
	number = {{arXiv}:2212.06011},
	publisher = {{arXiv}},
	author = {Zhong, Yaofeng Desmond and Zhang, Tongtao and Chakraborty, Amit and Dey, Biswadip},
	urldate = {2023-01-18},
	date = {2022-12-12},
	eprinttype = {arxiv},
	eprint = {2212.06011 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	annotation = {Contains many useful references to other recent works on Neural {ODEs}
},
	file = {arXiv Fulltext PDF:/home/pars/Coding/papers/Zotero/storage/NLEMHQZF/Zhong et al. - 2022 - A Neural ODE Interpretation of Transformer Layers.pdf:application/pdf;arXiv.org Snapshot:/home/pars/Coding/papers/Zotero/storage/NS27VX62/2212.html:text/html},
}

@misc{zheng_fast_2022,
	title = {Fast Sampling of Diffusion Models via Operator Learning},
	url = {http://arxiv.org/abs/2211.13449},
	doi = {10.48550/arXiv.2211.13449},
	abstract = {Diffusion models have found widespread adoption in various areas. However, sampling from them is slow because it involves emulating a reverse process with hundreds-to-thousands of network evaluations. Inspired by the success of neural operators in accelerating differential equations solving, we approach this problem by solving the underlying neural differential equation from an operator learning perspective. We examine probability flow {ODE} trajectories in diffusion models and observe a compact energy spectrum that can be learned efficiently in Fourier space. With this insight, we propose diffusion Fourier neural operator ({DFNO}) with temporal convolution in Fourier space to parameterize the operator that maps initial condition to the solution trajectory, which is a continuous function in time. {DFNO} can be applied to any diffusion model and generate high-quality samples in one model forward call. Our method achieves the state-of-the-art {FID} of 4.72 on {CIFAR}-10 using only one model evaluation.},
	number = {{arXiv}:2211.13449},
	publisher = {{arXiv}},
	author = {Zheng, Hongkai and Nie, Weili and Vahdat, Arash and Azizzadenesheli, Kamyar and Anandkumar, Anima},
	urldate = {2023-01-18},
	date = {2022-11-24},
	eprinttype = {arxiv},
	eprint = {2211.13449 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/pars/Coding/papers/Zotero/storage/JV92PIWR/Zheng et al. - 2022 - Fast Sampling of Diffusion Models via Operator Lea.pdf:application/pdf;arXiv.org Snapshot:/home/pars/Coding/papers/Zotero/storage/UL33VGHT/2211.html:text/html},
}

@misc{allauzen_experimental_2022,
	title = {Experimental study of Neural {ODE} training with adaptive solver for dynamical systems modeling},
	url = {http://arxiv.org/abs/2211.06972},
	doi = {10.48550/arXiv.2211.06972},
	abstract = {Neural Ordinary Differential Equations ({ODEs}) was recently introduced as a new family of neural network models, which relies on black-box {ODE} solvers for inference and training. Some {ODE} solvers called adaptive can adapt their evaluation strategy depending on the complexity of the problem at hand, opening great perspectives in machine learning. However, this paper describes a simple set of experiments to show why adaptive solvers cannot be seamlessly leveraged as a black-box for dynamical systems modelling. By taking the Lorenz'63 system as a showcase, we show that a naive application of the Fehlberg's method does not yield the expected results. Moreover, a simple workaround is proposed that assumes a tighter interaction between the solver and the training strategy. The code is available on github: https://github.com/Allauzen/adaptive-step-size-neural-ode},
	number = {{arXiv}:2211.06972},
	publisher = {{arXiv}},
	author = {Allauzen, Alexandre and Dardis, Thiago Petrilli Maffei and Plath, Hannah},
	urldate = {2023-01-18},
	date = {2022-11-13},
	eprinttype = {arxiv},
	eprint = {2211.06972 [nlin]},
	keywords = {Computer Science - Machine Learning, G.1, Nonlinear Sciences - Chaotic Dynamics},
	annotation = {Comment: Poster at the Neurips Workshop "The Symbiosis of Deep Learning and Differential Equations ({DLDE}) - {II}"},
	file = {arXiv Fulltext PDF:/home/pars/Coding/papers/Zotero/storage/HJ6I9RYS/Allauzen et al. - 2022 - Experimental study of Neural ODE training with ada.pdf:application/pdf;arXiv.org Snapshot:/home/pars/Coding/papers/Zotero/storage/QIF6JE85/2211.html:text/html},
}

@misc{hafner_mastering_2023,
	title = {Mastering Diverse Domains through World Models},
	url = {http://arxiv.org/abs/2301.04104},
	doi = {10.48550/arXiv.2301.04104},
	abstract = {General intelligence requires solving tasks across many domains. Current reinforcement learning algorithms carry this potential but are held back by the resources and knowledge required to tune them for new tasks. We present {DreamerV}3, a general and scalable algorithm based on world models that outperforms previous approaches across a wide range of domains with fixed hyperparameters. These domains include continuous and discrete actions, visual and low-dimensional inputs, 2D and 3D worlds, different data budgets, reward frequencies, and reward scales. We observe favorable scaling properties of {DreamerV}3, with larger models directly translating to higher data-efficiency and final performance. Applied out of the box, {DreamerV}3 is the first algorithm to collect diamonds in Minecraft from scratch without human data or curricula, a long-standing challenge in artificial intelligence. Our general algorithm makes reinforcement learning broadly applicable and allows scaling to hard decision-making problems.},
	number = {{arXiv}:2301.04104},
	publisher = {{arXiv}},
	author = {Hafner, Danijar and Pasukonis, Jurgis and Ba, Jimmy and Lillicrap, Timothy},
	urldate = {2023-01-18},
	date = {2023-01-10},
	eprinttype = {arxiv},
	eprint = {2301.04104 [cs, stat]},
	note = {version: 1},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	annotation = {Comment: Website: https://danijar.com/dreamerv3},
	file = {arXiv Fulltext PDF:/home/pars/Coding/papers/Zotero/storage/A4IQFHES/Hafner et al. - 2023 - Mastering Diverse Domains through World Models.pdf:application/pdf;arXiv.org Snapshot:/home/pars/Coding/papers/Zotero/storage/6AH5WZSM/2301.html:text/html},
}

@article{lecun_convolutional_1995,
	title = {Convolutional networks for images, speech, and time series},
	volume = {3361},
	pages = {1995},
	number = {10},
	journaltitle = {The handbook of brain theory and neural networks},
	author = {{LeCun}, Yann and Bengio, Yoshua},
	date = {1995},
	note = {Publisher: Cambridge, {MA} {USA}},
	keywords = {⛔ No {DOI} found},
	file = {Full Text:/home/pars/Coding/papers/Zotero/storage/RX8D4LME/LeCun and Bengio - 1995 - Convolutional networks for images, speech, and tim.pdf:application/pdf},
}

@article{metz_unrolled_2016,
	title = {Unrolled generative adversarial networks},
	journaltitle = {{arXiv} preprint {arXiv}:1611.02163},
	author = {Metz, Luke and Poole, Ben and Pfau, David and Sohl-Dickstein, Jascha},
	date = {2016},
	keywords = {{GAN}, ⛔ No {DOI} found},
	file = {Full Text:/home/pars/Coding/papers/Zotero/storage/F9R67M9H/Metz et al. - 2016 - Unrolled generative adversarial networks.pdf:application/pdf;Snapshot:/home/pars/Coding/papers/Zotero/storage/XGAWMNAP/1611.html:text/html},
}

@article{goodfellow_generative_2020,
	title = {Generative adversarial networks},
	volume = {63},
	doi = {10.1145/3422622},
	pages = {139--144},
	number = {11},
	journaltitle = {Communications of the {ACM}},
	author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
	date = {2020},
	note = {Publisher: {ACM} New York, {NY}, {USA}},
	keywords = {{GAN}},
	file = {Full Text:/home/pars/Coding/papers/Zotero/storage/UZWCTLRK/Goodfellow et al. - 2020 - Generative adversarial networks.pdf:application/pdf;Snapshot:/home/pars/Coding/papers/Zotero/storage/AMV9HJ6Z/3422622.html:text/html},
}

@article{creswell_generative_2018,
	title = {Generative Adversarial Networks: An Overview},
	volume = {35},
	issn = {1558-0792},
	doi = {10.1109/MSP.2017.2765202},
	shorttitle = {Generative Adversarial Networks},
	abstract = {Generative adversarial networks ({GANs}) provide a way to learn deep representations without extensively annotated training data. They achieve this by deriving backpropagation signals through a competitive process involving a pair of networks. The representations that can be learned by {GANs} may be used in a variety of applications, including image synthesis, semantic image editing, style transfer, image superresolution, and classification. The aim of this review article is to provide an overview of {GANs} for the signal processing community, drawing on familiar analogies and concepts where possible. In addition to identifying different methods for training and constructing {GANs}, we also point to remaining challenges in their theory and application.},
	pages = {53--65},
	number = {1},
	journaltitle = {{IEEE} Signal Processing Magazine},
	author = {Creswell, Antonia and White, Tom and Dumoulin, Vincent and Arulkumaran, Kai and Sengupta, Biswa and Bharath, Anil A.},
	date = {2018-01},
	note = {Conference Name: {IEEE} Signal Processing Magazine},
	keywords = {Convolutional codes, Data models, Generators, Image resolution, Machine learning, Semantics, Signal resolution, Training data},
	file = {IEEE Xplore Abstract Record:/home/pars/Coding/papers/Zotero/storage/EUB2SM3B/8253599.html:text/html;IEEE Xplore Full Text PDF:/home/pars/Coding/papers/Zotero/storage/VFXGNHK4/Creswell et al. - 2018 - Generative Adversarial Networks An Overview.pdf:application/pdf},
}

@article{gui_review_2021,
	title = {A Review on Generative Adversarial Networks: Algorithms, Theory, and Applications},
	issn = {1558-2191},
	doi = {10.1109/TKDE.2021.3130191},
	shorttitle = {A Review on Generative Adversarial Networks},
	abstract = {Generative adversarial networks ({GANs}) have recently become a hot research topic; however, they have been studied since 2014, and a large number of algorithms have been proposed. However, few comprehensive studies exist explaining the connections among different {GANs} variants and how they have evolved. In this paper, we attempt to provide a review of the various {GANs} methods from the perspectives of algorithms, theory, and applications. First, the motivations, mathematical representations, and structures of most {GANs} algorithms are introduced in detail and we compare their commonalities and differences. Second, theoretical issues related to {GANs} are investigated. Finally, typical applications of {GANs} in image processing and computer vision, natural language processing, music, speech and audio, medical field, and data science are discussed.},
	pages = {1--1},
	journaltitle = {{IEEE} Transactions on Knowledge and Data Engineering},
	author = {Gui, Jie and Sun, Zhenan and Wen, Yonggang and Tao, Dacheng and Ye, Jieping},
	date = {2021},
	note = {Conference Name: {IEEE} Transactions on Knowledge and Data Engineering},
	keywords = {Data models, Generators, Algorithm, Applications, Deep Learning, Generative adversarial networks, Generative Adversarial Networks, Inference algorithms, Linear programming, Machine learning algorithms, Natural language processing, Theory},
	file = {IEEE Xplore Abstract Record:/home/pars/Coding/papers/Zotero/storage/5PHGY4D6/9625798.html:text/html;IEEE Xplore Full Text PDF:/home/pars/Coding/papers/Zotero/storage/PASKZY7K/Gui et al. - 2021 - A Review on Generative Adversarial Networks Algor.pdf:application/pdf},
}

@inproceedings{he_deep_2016,
	title = {Deep residual learning for image recognition},
	pages = {770--778},
	booktitle = {Proceedings of the {IEEE} conference on computer vision and pattern recognition},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	date = {2016},
	keywords = {⛔ No {DOI} found},
	file = {Full Text:/home/pars/Coding/papers/Zotero/storage/N8NEAARG/He et al. - 2016 - Deep residual learning for image recognition.pdf:application/pdf;Snapshot:/home/pars/Coding/papers/Zotero/storage/IL5R6F4I/He_Deep_Residual_Learning_CVPR_2016_paper.html:text/html},
}

@article{krizhevsky_imagenet_2012,
	title = {Imagenet classification with deep convolutional neural networks},
	volume = {60},
	doi = {10.1145/3065386},
	pages = {84--90},
	number = {6},
	journaltitle = {Communications of the {ACM}},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
	date = {2012},
	note = {Publisher: {AcM} New York, {NY}, {USA}},
	file = {Full Text:/home/pars/Coding/papers/Zotero/storage/EMU6TIK8/Krizhevsky et al. - 2017 - Imagenet classification with deep convolutional ne.pdf:application/pdf;Snapshot:/home/pars/Coding/papers/Zotero/storage/G4III8CK/3065386.html:text/html},
}

@inproceedings{hoffmann_empirical_2022,
	title = {An empirical analysis of compute-optimal large language model training},
	url = {https://openreview.net/forum?id=iBBcRUlOAPR},
	abstract = {We investigate the optimal model size and number of tokens for training a transformer language model under a given compute budget. We find that current large language models are significantly undertrained, a consequence of the recent focus on scaling language models whilst keeping the amount of training data constant. By training over 400 language models ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens, we find that for compute-optimal training, the model size and the number of training tokens should be scaled equally: for every doubling of model size the number of training tokens should also be doubled. We test this hypothesis by training a predicted compute-optimal model, Chinchilla, that uses the same compute budget as Gopher but with 70B parameters and 4\${\textbackslash}times\$ more data. Chinchilla uniformly and significantly {outperformsGopher} (280B), {GPT}-3 (175B), Jurassic-1 (178B), and Megatron-Turing {NLG} (530B) on a large range of downstream evaluation tasks. This also means that Chinchilla uses substantially less compute for fine-tuning and inference, greatly facilitating downstream usage. As a highlight, Chinchilla reaches a state-of-the-art average accuracy of 67.5\% on the {MMLU} benchmark, a 7\% improvement over Gopher.},
	eventtitle = {Advances in Neural Information Processing Systems},
	author = {Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and Hennigan, Tom and Noland, Eric and Millican, Katherine and Driessche, George van den and Damoc, Bogdan and Guy, Aurelia and Osindero, Simon and Simonyan, Karen and Elsen, Erich and Vinyals, Oriol and Rae, Jack William and Sifre, Laurent},
	urldate = {2023-01-18},
	date = {2022-10-31},
	langid = {english},
	keywords = {{DeepMind}},
	file = {Full Text PDF:/home/pars/Coding/papers/Zotero/storage/GDDD5QJD/Hoffmann et al. - 2022 - An empirical analysis of compute-optimal large lan.pdf:application/pdf},
}

@misc{alayrac_flamingo_2022,
	title = {Flamingo: a Visual Language Model for Few-Shot Learning},
	url = {http://arxiv.org/abs/2204.14198},
	doi = {10.48550/arXiv.2204.14198},
	shorttitle = {Flamingo},
	abstract = {Building models that can be rapidly adapted to novel tasks using only a handful of annotated examples is an open challenge for multimodal machine learning research. We introduce Flamingo, a family of Visual Language Models ({VLM}) with this ability. We propose key architectural innovations to: (i) bridge powerful pretrained vision-only and language-only models, (ii) handle sequences of arbitrarily interleaved visual and textual data, and (iii) seamlessly ingest images or videos as inputs. Thanks to their flexibility, Flamingo models can be trained on large-scale multimodal web corpora containing arbitrarily interleaved text and images, which is key to endow them with in-context few-shot learning capabilities. We perform a thorough evaluation of our models, exploring and measuring their ability to rapidly adapt to a variety of image and video tasks. These include open-ended tasks such as visual question-answering, where the model is prompted with a question which it has to answer; captioning tasks, which evaluate the ability to describe a scene or an event; and close-ended tasks such as multiple-choice visual question-answering. For tasks lying anywhere on this spectrum, a single Flamingo model can achieve a new state of the art with few-shot learning, simply by prompting the model with task-specific examples. On numerous benchmarks, Flamingo outperforms models fine-tuned on thousands of times more task-specific data.},
	number = {{arXiv}:2204.14198},
	publisher = {{arXiv}},
	author = {Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katie and Reynolds, Malcolm and Ring, Roman and Rutherford, Eliza and Cabi, Serkan and Han, Tengda and Gong, Zhitao and Samangooei, Sina and Monteiro, Marianne and Menick, Jacob and Borgeaud, Sebastian and Brock, Andrew and Nematzadeh, Aida and Sharifzadeh, Sahand and Binkowski, Mikolaj and Barreira, Ricardo and Vinyals, Oriol and Zisserman, Andrew and Simonyan, Karen},
	urldate = {2023-01-18},
	date = {2022-11-15},
	eprinttype = {arxiv},
	eprint = {2204.14198 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, {DeepMind}},
	annotation = {Comment: 54 pages. In Proceedings of Neural Information Processing Systems ({NeurIPS}) 2022},
	file = {arXiv Fulltext PDF:/home/pars/Coding/papers/Zotero/storage/ENK2BSNT/Alayrac et al. - 2022 - Flamingo a Visual Language Model for Few-Shot Lea.pdf:application/pdf;arXiv.org Snapshot:/home/pars/Coding/papers/Zotero/storage/99BVXLGT/2204.html:text/html},
}

@article{chen_residual_2019,
	title = {Residual flows for invertible generative modeling},
	volume = {32},
	journaltitle = {Advances in Neural Information Processing Systems},
	author = {Chen, Ricky {TQ} and Behrmann, Jens and Duvenaud, David K. and Jacobsen, Jörn-Henrik},
	date = {2019},
	keywords = {⛔ No {DOI} found},
	file = {Full Text:/home/pars/Coding/papers/Zotero/storage/IX22TTHC/Chen et al. - 2019 - Residual flows for invertible generative modeling.pdf:application/pdf;Snapshot:/home/pars/Coding/papers/Zotero/storage/T62PKAFY/5d0d5594d24f0f955548f0fc0ff83d10-Abstract.html:text/html},
}

@article{devlin_bert_2018,
	title = {Bert: Pre-training of deep bidirectional transformers for language understanding},
	shorttitle = {Bert},
	journaltitle = {{arXiv} preprint {arXiv}:1810.04805},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	date = {2018},
	keywords = {⛔ No {DOI} found},
	file = {Full Text:/home/pars/Coding/papers/Zotero/storage/82FDKQ7Z/Devlin et al. - 2018 - Bert Pre-training of deep bidirectional transform.pdf:application/pdf;Snapshot:/home/pars/Coding/papers/Zotero/storage/PIRVPC24/1810.html:text/html},
}

@inproceedings{qi_pointnet_2017,
	title = {Pointnet: Deep learning on point sets for 3d classification and segmentation},
	shorttitle = {Pointnet},
	pages = {652--660},
	booktitle = {Proceedings of the {IEEE} conference on computer vision and pattern recognition},
	author = {Qi, Charles R. and Su, Hao and Mo, Kaichun and Guibas, Leonidas J.},
	date = {2017},
	keywords = {⛔ No {DOI} found},
	file = {Full Text:/home/pars/Coding/papers/Zotero/storage/JFCZMD3G/Qi et al. - 2017 - Pointnet Deep learning on point sets for 3d class.pdf:application/pdf;Snapshot:/home/pars/Coding/papers/Zotero/storage/43D3D3AA/Qi_PointNet_Deep_Learning_CVPR_2017_paper.html:text/html},
}

@article{hamilton_inductive_2017,
	title = {Inductive representation learning on large graphs},
	volume = {30},
	journaltitle = {Advances in neural information processing systems},
	author = {Hamilton, Will and Ying, Zhitao and Leskovec, Jure},
	date = {2017},
	keywords = {⛔ No {DOI} found},
	file = {Full Text:/home/pars/Coding/papers/Zotero/storage/KP9YWUKF/Hamilton et al. - 2017 - Inductive representation learning on large graphs.pdf:application/pdf;Snapshot:/home/pars/Coding/papers/Zotero/storage/WV6PSYVA/5dd9db5e033da9c6fb5ba83c7a7ebea9-Abstract.html:text/html},
}

@article{qi_pointnet_2017-1,
	title = {Pointnet++: Deep hierarchical feature learning on point sets in a metric space},
	volume = {30},
	shorttitle = {Pointnet++},
	journaltitle = {Advances in neural information processing systems},
	author = {Qi, Charles Ruizhongtai and Yi, Li and Su, Hao and Guibas, Leonidas J.},
	date = {2017},
	keywords = {⛔ No {DOI} found},
	file = {Full Text:/home/pars/Coding/papers/Zotero/storage/GLSM6UB2/Qi et al. - 2017 - Pointnet++ Deep hierarchical feature learning on .pdf:application/pdf;Snapshot:/home/pars/Coding/papers/Zotero/storage/HDH5SPQ8/d8bf84be3800d12f74d8b05e9b89836f-Abstract.html:text/html},
}

@article{srivastava_dropout_2014,
	title = {Dropout: a simple way to prevent neural networks from overfitting},
	volume = {15},
	shorttitle = {Dropout},
	pages = {1929--1958},
	number = {1},
	journaltitle = {The journal of machine learning research},
	author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
	date = {2014},
	note = {Publisher: {JMLR}. org},
	keywords = {⛔ No {DOI} found},
	file = {Full Text:/home/pars/Coding/papers/Zotero/storage/BQABCS47/Srivastava et al. - 2014 - Dropout a simple way to prevent neural networks f.pdf:application/pdf},
}

@article{soleimani_scalable_2017,
	title = {Scalable joint models for reliable uncertainty-aware event prediction},
	volume = {40},
	doi = {10.1109/TPAMI.2017.2742504},
	pages = {1948--1963},
	number = {8},
	journaltitle = {{IEEE} transactions on pattern analysis and machine intelligence},
	author = {Soleimani, Hossein and Hensman, James and Saria, Suchi},
	date = {2017},
	note = {Publisher: {IEEE}},
	file = {Full Text:/home/pars/Coding/papers/Zotero/storage/B5QMF5Z3/Soleimani et al. - 2017 - Scalable joint models for reliable uncertainty-awa.pdf:application/pdf;Snapshot:/home/pars/Coding/papers/Zotero/storage/C4PRBEK7/8013802.html:text/html},
}

@article{raissi_multistep_2018,
	title = {Multistep neural networks for data-driven discovery of nonlinear dynamical systems},
	journaltitle = {{arXiv} preprint {arXiv}:1801.01236},
	author = {Raissi, Maziar and Perdikaris, Paris and Karniadakis, George Em},
	date = {2018},
	keywords = {⛔ No {DOI} found},
	file = {Full Text:/home/pars/Coding/papers/Zotero/storage/WSH57EK3/Raissi et al. - 2018 - Multistep neural networks for data-driven discover.pdf:application/pdf;Snapshot:/home/pars/Coding/papers/Zotero/storage/CEMUY2Y4/1801.html:text/html},
}

@article{wiewel_latent_2019,
	title = {Latent Space Physics: Towards Learning the Temporal Evolution of Fluid Flow},
	volume = {38},
	issn = {1467-8659},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.13620},
	doi = {10.1111/cgf.13620},
	shorttitle = {Latent Space Physics},
	abstract = {We propose a method for the data-driven inference of temporal evolutions of physical functions with deep learning. More specifically, we target fluid flow problems, and we propose a novel {LSTM}-based approach to predict the changes of the pressure field over time. The central challenge in this context is the high dimensionality of Eulerian space-time data sets. We demonstrate for the first time that dense 3D+time functions of physics system can be predicted within the latent spaces of neural networks, and we arrive at a neural-network based simulation algorithm with significant practical speed-ups. We highlight the capabilities of our method with a series of complex liquid simulations, and with a set of single-phase buoyancy simulations. With a set of trained networks, our method is more than two orders of magnitudes faster than a traditional pressure solver. Additionally, we present and discuss a series of detailed evaluations for the different components of our algorithm.},
	pages = {71--82},
	number = {2},
	journaltitle = {Computer Graphics Forum},
	author = {Wiewel, S. and Becher, M. and Thuerey, N.},
	urldate = {2023-01-19},
	date = {2019},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/cgf.13620},
	keywords = {• Computing methodologies → Neural networks, {CCS} Concepts, Physical simulation},
	file = {Full Text PDF:/home/pars/Coding/papers/Zotero/storage/BYBGRTNZ/Wiewel et al. - 2019 - Latent Space Physics Towards Learning the Tempora.pdf:application/pdf;Snapshot:/home/pars/Coding/papers/Zotero/storage/XIT7AR53/cgf.html:text/html},
}

@article{soleimani_treatment-response_2017,
	title = {Treatment-response models for counterfactual reasoning with continuous-time, continuous-valued interventions},
	journaltitle = {{arXiv} preprint {arXiv}:1704.02038},
	author = {Soleimani, Hossein and Subbaswamy, Adarsh and Saria, Suchi},
	date = {2017},
	keywords = {⛔ No {DOI} found},
	file = {Full Text:/home/pars/Coding/papers/Zotero/storage/A8MX4PJL/Soleimani et al. - 2017 - Treatment-response models for counterfactual reaso.pdf:application/pdf;Snapshot:/home/pars/Coding/papers/Zotero/storage/97GXKI6Z/1704.html:text/html},
}

@misc{millidge_neural_2021,
	title = {Neural Kalman Filtering},
	url = {http://arxiv.org/abs/2102.10021},
	doi = {10.48550/arXiv.2102.10021},
	abstract = {The Kalman filter is a fundamental filtering algorithm that fuses noisy sensory data, a previous state estimate, and a dynamics model to produce a principled estimate of the current state. It assumes, and is optimal for, linear models and white Gaussian noise. Due to its relative simplicity and general effectiveness, the Kalman filter is widely used in engineering applications. Since many sensory problems the brain faces are, at their core, filtering problems, it is possible that the brain possesses neural circuitry that implements equivalent computations to the Kalman filter. The standard approach to Kalman filtering requires complex matrix computations that are unlikely to be directly implementable in neural circuits. In this paper, we show that a gradient-descent approximation to the Kalman filter requires only local computations with variance weighted prediction errors. Moreover, we show that it is possible under the same scheme to adaptively learn the dynamics model with a learning rule that corresponds directly to Hebbian plasticity. We demonstrate the performance of our method on a simple Kalman filtering task, and propose a neural implementation of the required equations.},
	number = {{arXiv}:2102.10021},
	publisher = {{arXiv}},
	author = {Millidge, Beren and Tschantz, Alexander and Seth, Anil and Buckley, Christopher},
	urldate = {2023-01-19},
	date = {2021-04-29},
	eprinttype = {arxiv},
	eprint = {2102.10021 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing},
	annotation = {Comment: 17-02-21 initial upload; 29-04-21 minor fixes},
	file = {arXiv Fulltext PDF:/home/pars/Coding/papers/Zotero/storage/TNC5DI2I/Millidge et al. - 2021 - Neural Kalman Filtering.pdf:application/pdf;arXiv.org Snapshot:/home/pars/Coding/papers/Zotero/storage/ZEX6TJS3/2102.html:text/html},
}

@article{gordon_novel_1993,
	title = {Novel approach to nonlinear/non-Gaussian Bayesian state estimation},
	volume = {140},
	issn = {0956375X},
	url = {https://digital-library.theiet.org/content/journals/10.1049/ip-f-2.1993.0015},
	doi = {10.1049/ip-f-2.1993.0015},
	abstract = {An algorithm, the bootstrap filter, is proposed for implementing recursive Bayesian filters. The required density of the state vector is represented as a set of random samples, which are updated and propagated by the algorithm. The method is not restricted by assumptions of linearity or Gaussian noise: it may be applied to any state transition or measurement model. A simulation example of the bearings only tracking problem is presented. This simulation includes schemes for improving the efficiency of the basic algorithm. For this example, the performance of the bootstrap filter is greatly superior to the standard extended Kalman filter.},
	pages = {107},
	number = {2},
	journaltitle = {{IEE} Proceedings F Radar and Signal Processing},
	shortjournal = {{IEE} Proc. F Radar Signal Process. {UK}},
	author = {Gordon, N.J. and Salmond, D.J. and Smith, A.F.M.},
	urldate = {2023-01-19},
	date = {1993},
	langid = {english},
	file = {Gordon et al. - 1993 - Novel approach to nonlinearnon-Gaussian Bayesian .pdf:/home/pars/Coding/papers/Zotero/storage/ICYN59W3/Gordon et al. - 1993 - Novel approach to nonlinearnon-Gaussian Bayesian .pdf:application/pdf},
}

@misc{hinton_distilling_2015,
	title = {Distilling the Knowledge in a Neural Network},
	url = {http://arxiv.org/abs/1503.02531},
	doi = {10.48550/arXiv.1503.02531},
	abstract = {A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on {MNIST} and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.},
	number = {{arXiv}:1503.02531},
	publisher = {{arXiv}},
	author = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
	urldate = {2023-01-19},
	date = {2015-03-09},
	eprinttype = {arxiv},
	eprint = {1503.02531 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	annotation = {Comment: {NIPS} 2014 Deep Learning Workshop},
	file = {arXiv Fulltext PDF:/home/pars/Coding/papers/Zotero/storage/D7G7NYJW/Hinton et al. - 2015 - Distilling the Knowledge in a Neural Network.pdf:application/pdf;arXiv.org Snapshot:/home/pars/Coding/papers/Zotero/storage/2WBUYT9V/1503.html:text/html},
}

@misc{polino_model_2018,
	title = {Model compression via distillation and quantization},
	url = {http://arxiv.org/abs/1802.05668},
	doi = {10.48550/arXiv.1802.05668},
	abstract = {Deep neural networks ({DNNs}) continue to make significant advances, solving tasks from image classification to translation or reinforcement learning. One aspect of the field receiving considerable attention is efficiently executing deep models in resource-constrained environments, such as mobile or embedded devices. This paper focuses on this problem, and proposes two new compression methods, which jointly leverage weight quantization and distillation of larger teacher networks into smaller student networks. The first method we propose is called quantized distillation and leverages distillation during the training process, by incorporating distillation loss, expressed with respect to the teacher, into the training of a student network whose weights are quantized to a limited set of levels. The second method, differentiable quantization, optimizes the location of quantization points through stochastic gradient descent, to better fit the behavior of the teacher model. We validate both methods through experiments on convolutional and recurrent architectures. We show that quantized shallow students can reach similar accuracy levels to full-precision teacher models, while providing order of magnitude compression, and inference speedup that is linear in the depth reduction. In sum, our results enable {DNNs} for resource-constrained environments to leverage architecture and accuracy advances developed on more powerful devices.},
	number = {{arXiv}:1802.05668},
	publisher = {{arXiv}},
	author = {Polino, Antonio and Pascanu, Razvan and Alistarh, Dan},
	urldate = {2023-01-19},
	date = {2018-02-15},
	eprinttype = {arxiv},
	eprint = {1802.05668 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	annotation = {Comment: 21 pages, published as a conference paper at {ICLR}2018},
	file = {arXiv Fulltext PDF:/home/pars/Coding/papers/Zotero/storage/2IEU5WPW/Polino et al. - 2018 - Model compression via distillation and quantizatio.pdf:application/pdf;arXiv.org Snapshot:/home/pars/Coding/papers/Zotero/storage/TZ2MKEKA/1802.html:text/html},
}

@article{landauer_deep_2022,
	title = {Deep Learning for Anomaly Detection in Log Data: A Survey},
	shorttitle = {Deep Learning for Anomaly Detection in Log Data},
	journaltitle = {{arXiv} preprint {arXiv}:2207.03820},
	author = {Landauer, Max and Onder, Sebastian and Skopik, Florian and Wurzenberger, Markus},
	date = {2022},
	keywords = {⛔ No {DOI} found},
	file = {Full Text:/home/pars/Coding/papers/Zotero/storage/BIVJQKHW/Landauer et al. - 2022 - Deep Learning for Anomaly Detection in Log Data A.pdf:application/pdf;Snapshot:/home/pars/Coding/papers/Zotero/storage/5MDBC3GU/2207.html:text/html},
}

@article{xu_hyperspectral_2022,
	title = {Hyperspectral Anomaly Detection based on Machine Learning: An Overview},
	doi = {10.1109/JSTARS.2022.3167830},
	shorttitle = {Hyperspectral Anomaly Detection based on Machine Learning},
	journaltitle = {{IEEE} Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
	author = {Xu, Yichu and Zhang, Lefei and Du, Bo and Zhang, Liangpei},
	date = {2022},
	note = {Publisher: {IEEE}},
	file = {Full Text:/home/pars/Coding/papers/Zotero/storage/5KPBPA46/Xu et al. - 2022 - Hyperspectral Anomaly Detection based on Machine L.pdf:application/pdf;Snapshot:/home/pars/Coding/papers/Zotero/storage/9NYJP53B/9760098.html:text/html},
}

@inproceedings{bogdoll_anomaly_2022,
	title = {Anomaly Detection in Autonomous Driving: A Survey},
	shorttitle = {Anomaly Detection in Autonomous Driving},
	pages = {4488--4499},
	booktitle = {Proceedings of the {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition},
	author = {Bogdoll, Daniel and Nitsche, Maximilian and Zöllner, J. Marius},
	date = {2022},
	keywords = {⛔ No {DOI} found},
	file = {Full Text:/home/pars/Coding/papers/Zotero/storage/4APT9TH3/Bogdoll et al. - 2022 - Anomaly Detection in Autonomous Driving A Survey.pdf:application/pdf;Snapshot:/home/pars/Coding/papers/Zotero/storage/67U8YQMX/Bogdoll_Anomaly_Detection_in_Autonomous_Driving_A_Survey_CVPRW_2022_paper.html:text/html},
}

@article{schmidl_anomaly_2022,
	title = {Anomaly detection in time series: a comprehensive evaluation},
	volume = {15},
	doi = {10.14778/3538598.3538602},
	shorttitle = {Anomaly detection in time series},
	pages = {1779--1797},
	number = {9},
	journaltitle = {Proceedings of the {VLDB} Endowment},
	author = {Schmidl, Sebastian and Wenig, Phillip and Papenbrock, Thorsten},
	date = {2022},
	note = {Publisher: {VLDB} Endowment},
	file = {Full Text:/home/pars/Coding/papers/Zotero/storage/Z3XUICRY/Schmidl et al. - 2022 - Anomaly detection in time series a comprehensive .pdf:application/pdf;Snapshot:/home/pars/Coding/papers/Zotero/storage/4QZMEMHW/3538598.html:text/html},
}

@article{xu_hyperspectral_2022-1,
	title = {Hyperspectral Anomaly Detection Based on Machine Learning: An Overview},
	volume = {15},
	issn = {2151-1535},
	doi = {10.1109/JSTARS.2022.3167830},
	shorttitle = {Hyperspectral Anomaly Detection Based on Machine Learning},
	abstract = {Hyperspectral anomaly detection ({HAD}) is an important hyperspectral image application. {HAD} can find pixels with anomalous spectral signatures compared with their neighbor background without any prior information. While most of the existed researches are related to statistic-based and distance-based techniques, by summarizing the background samples with certain models, and then, finding the very few outliers by various distance metrics, this review focuses on the {HAD} based on machine learning methods, which have witnessed remarkable progress in the recent years. In particular, these studies can generally be grouped into the traditional machine learning and deep-learning-based methods. Several representative {HAD} methods, including both traditional machine and deep-learning-based methods, are then conducted on four real {HSIs} in the experiments. Finally, conclusions regarding {HAD} are summarized, and prospects and future development direction are discussed.},
	pages = {3351--3364},
	journaltitle = {{IEEE} Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
	author = {Xu, Yichu and Zhang, Lefei and Du, Bo and Zhang, Liangpei},
	date = {2022},
	note = {Conference Name: {IEEE} Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
	keywords = {Anomaly detection, deep learning, Dictionaries, hyperspectral imagery, Hyperspectral imaging, Kernel, Learning systems, machine learning, Manifolds, Task analysis},
	file = {IEEE Xplore Abstract Record:/home/pars/Coding/papers/Zotero/storage/WNTM6ACZ/9760098.html:text/html;IEEE Xplore Full Text PDF:/home/pars/Coding/papers/Zotero/storage/YAYSHHLN/Xu et al. - 2022 - Hyperspectral Anomaly Detection Based on Machine L.pdf:application/pdf},
}

@article{jain_autonomy_2021,
	title = {Autonomy 2.0: Why is self-driving always 5 years away?},
	shorttitle = {Autonomy 2.0},
	journaltitle = {{arXiv} preprint {arXiv}:2107.08142},
	author = {Jain, Ashesh and Del Pero, Luca and Grimmett, Hugo and Ondruska, Peter},
	date = {2021},
	keywords = {⛔ No {DOI} found},
	file = {Full Text:/home/pars/Coding/papers/Zotero/storage/UNI8QDVE/Jain et al. - 2021 - Autonomy 2.0 Why is self-driving always 5 years a.pdf:application/pdf;Snapshot:/home/pars/Coding/papers/Zotero/storage/LVDV972N/2107.html:text/html},
}

@article{breitenstein_corner_2021,
	title = {Corner cases for visual perception in automated driving: Some guidance on detection approaches},
	shorttitle = {Corner cases for visual perception in automated driving},
	journaltitle = {{arXiv} preprint {arXiv}:2102.05897},
	author = {Breitenstein, Jasmin and Termöhlen, Jan-Aike and Lipinski, Daniel and Fingscheidt, Tim},
	date = {2021},
	keywords = {⛔ No {DOI} found},
	file = {Full Text:/home/pars/Coding/papers/Zotero/storage/UMWTL4JG/Breitenstein et al. - 2021 - Corner cases for visual perception in automated dr.pdf:application/pdf;Snapshot:/home/pars/Coding/papers/Zotero/storage/XWETYEYN/2102.html:text/html},
}

@inproceedings{breitenstein_systematization_2020,
	title = {Systematization of Corner Cases for Visual Perception in Automated Driving},
	doi = {10.1109/IV47402.2020.9304789},
	abstract = {One major task in automated driving is the development of robust and safe visual perception modules. It is of utmost importance that visual perception reacts adequately to so-called corner cases, which range from overexposure of the image sensor to unexpected and potentially dangerous traffic situations. Their detection thus has high significance both as an online system in the intelligent vehicle, but also in the extraction of relevant training and test data for perception modules. In this paper, we provide a systematization of corner cases for visual perception in automated driving, with the categories being structured by detection complexity. Furthermore, we discuss existing metrics and datasets which can be used for the evaluation of corner case detection methods depending on their suitability to provide beneficial information for the various categories.},
	eventtitle = {2020 {IEEE} Intelligent Vehicles Symposium ({IV})},
	pages = {1257--1264},
	booktitle = {2020 {IEEE} Intelligent Vehicles Symposium ({IV})},
	author = {Breitenstein, Jasmin and Termöhlen, Jan-Aike and Lipinski, Daniel and Fingscheidt, Tim},
	date = {2020-10},
	note = {{ISSN}: 2642-7214},
	keywords = {Semantics, Automobiles, Complexity theory, Roads, Training, Visual perception, Visualization},
	file = {IEEE Xplore Abstract Record:/home/pars/Coding/papers/Zotero/storage/49KFAI8V/stamp.html:text/html;IEEE Xplore Full Text PDF:/home/pars/Coding/papers/Zotero/storage/NB5L727K/Breitenstein et al. - 2020 - Systematization of Corner Cases for Visual Percept.pdf:application/pdf},
}

@inproceedings{he_mask_2017,
	title = {Mask r-cnn},
	pages = {2961--2969},
	booktitle = {Proceedings of the {IEEE} international conference on computer vision},
	author = {He, Kaiming and Gkioxari, Georgia and Dollár, Piotr and Girshick, Ross},
	date = {2017},
	keywords = {⛔ No {DOI} found},
	file = {Full Text:/home/pars/Coding/papers/Zotero/storage/38RL7AT3/He et al. - 2017 - Mask r-cnn.pdf:application/pdf;Snapshot:/home/pars/Coding/papers/Zotero/storage/62MZST2E/He_Mask_R-CNN_ICCV_2017_paper.html:text/html},
}

@article{ren_faster_2015,
	title = {Faster r-cnn: Towards real-time object detection with region proposal networks},
	volume = {28},
	shorttitle = {Faster r-cnn},
	journaltitle = {Advances in neural information processing systems},
	author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
	date = {2015},
	keywords = {⛔ No {DOI} found},
	file = {Full Text:/home/pars/Coding/papers/Zotero/storage/ZDJF38AC/Ren et al. - 2015 - Faster r-cnn Towards real-time object detection w.pdf:application/pdf;Snapshot:/home/pars/Coding/papers/Zotero/storage/65YH7VCM/14bfa6bb14875e45bba028a21ed38046-Abstract.html:text/html},
}

@inproceedings{girshick_fast_2015,
	title = {Fast r-cnn},
	pages = {1440--1448},
	booktitle = {Proceedings of the {IEEE} international conference on computer vision},
	author = {Girshick, Ross},
	date = {2015},
	keywords = {⛔ No {DOI} found},
	file = {Full Text:/home/pars/Coding/papers/Zotero/storage/NEIX5RGA/Girshick - 2015 - Fast r-cnn.pdf:application/pdf;Snapshot:/home/pars/Coding/papers/Zotero/storage/WBAU6B8F/Girshick_Fast_R-CNN_ICCV_2015_paper.html:text/html},
}

@article{lu_learning_2019,
	title = {Learning under Concept Drift: A Review},
	volume = {31},
	issn = {1558-2191},
	doi = {10.1109/TKDE.2018.2876857},
	shorttitle = {Learning under Concept Drift},
	abstract = {Concept drift describes unforeseeable changes in the underlying distribution of streaming data overtime. Concept drift research involves the development of methodologies and techniques for drift detection, understanding, and adaptation. Data analysis has revealed that machine learning in a concept drift environment will result in poor learning results if the drift is not addressed. To help researchers identify which research topics are significant and how to apply related techniques in data analysis tasks, it is necessary that a high quality, instructive review of current research developments and trends in the concept drift field is conducted. In addition, due to the rapid development of concept drift in recent years, the methodologies of learning under concept drift have become noticeably systematic, unveiling a framework which has not been mentioned in literature. This paper reviews over 130 high quality publications in concept drift related research areas, analyzes up-to-date developments in methodologies and techniques, and establishes a framework of learning under concept drift including three main components: concept drift detection, concept drift understanding, and concept drift adaptation. This paper lists and discusses 10 popular synthetic datasets and 14 publicly available benchmark datasets used for evaluating the performance of learning algorithms aiming at handling concept drift. Also, concept drift related research directions are covered and discussed. By providing state-of-the-art knowledge, this survey will directly support researchers in their understanding of research developments in the field of learning under concept drift.},
	pages = {2346--2363},
	number = {12},
	journaltitle = {{IEEE} Transactions on Knowledge and Data Engineering},
	author = {Lu, Jie and Liu, Anjin and Dong, Fan and Gu, Feng and Gama, João and Zhang, Guangquan},
	date = {2019-12},
	note = {Conference Name: {IEEE} Transactions on Knowledge and Data Engineering},
	keywords = {Data models, Machine learning, adaptive learning, Big Data, Cameras, change detection, Concept drift, Data analysis, data streams, Market research, Mobile handsets},
	file = {IEEE Xplore Abstract Record:/home/pars/Coding/papers/Zotero/storage/HUEBCI5W/stamp.html:text/html;IEEE Xplore Full Text PDF:/home/pars/Coding/papers/Zotero/storage/BP9CELA8/Lu et al. - 2019 - Learning under Concept Drift A Review.pdf:application/pdf},
}

@article{barros_large-scale_2018,
	title = {A large-scale comparison of concept drift detectors},
	volume = {451},
	doi = {10.1016/j.ins.2018.04.014},
	pages = {348--370},
	journaltitle = {Information Sciences},
	author = {Barros, Roberto Souto Maior and Santos, Silas Garrido T. Carvalho},
	date = {2018},
	note = {Publisher: Elsevier},
	file = {Full Text:/home/pars/Coding/papers/Zotero/storage/JKDJKZPF/Barros and Santos - 2018 - A large-scale comparison of concept drift detector.pdf:application/pdf;Snapshot:/home/pars/Coding/papers/Zotero/storage/P8LYH7R8/S0020025518302743.html:text/html},
}

@article{lobo_curie_2021,
	title = {{CURIE}: a cellular automaton for concept drift detection},
	volume = {35},
	doi = {10.1007/s10618-021-00776-2},
	shorttitle = {{CURIE}},
	pages = {2655--2678},
	number = {6},
	journaltitle = {Data Mining and Knowledge Discovery},
	author = {Lobo, Jesus L. and Del Ser, Javier and Osaba, Eneko and Bifet, Albert and Herrera, Francisco},
	date = {2021},
	note = {Publisher: Springer},
	file = {Full Text:/home/pars/Coding/papers/Zotero/storage/BMLHLYZN/Lobo et al. - 2021 - CURIE a cellular automaton for concept drift dete.pdf:application/pdf;Snapshot:/home/pars/Coding/papers/Zotero/storage/T2UYIITG/s10618-021-00776-2.html:text/html},
}

@article{baburoglu_novel_2021,
	title = {Novel hybrid pair recommendations based on a large-scale comparative study of concept drift detection},
	volume = {163},
	doi = {10.1016/j.eswa.2020.113786},
	pages = {113786},
	journaltitle = {Expert Systems with Applications},
	author = {Babüroğlu, Elif Selen and Durmuşoğlu, Alptekin and Dereli, Türkay},
	date = {2021},
	note = {Publisher: Elsevier},
	file = {Snapshot:/home/pars/Coding/papers/Zotero/storage/XN97UTJH/S0957417420306102.html:text/html},
}

@article{yang_1_nodate,
	title = {1 Loss of epigenetic information as a cause of mammalian aging},
	author = {Yang, Jae-Hyun and Hayano, Motoshi and Griffin, Patrick T and Amorim, Joao A and Apostolides, John K and Blanchette, Marco and Munding, Elizabeth M and Bhakta, Mital and Salfati, Elias L and Lu, Yuancheng and Vera, Daniel L and Ross, Jaime M and Coppotelli, Giuseppe and Chew, Ching and Guo, Wei and Yang, Xiaojing and Meer, Margarita V and Tian, Xiao and Dou, Zhixun and Pippin, Jeffrey W and Creswell, Michael L and Mitchell, Sarah J and Das, Abhirup and Thakur, Sachin and Kane, Alice E and Su, Qiao and Mohri, Yasuaki and Nishimura, Emi K and Schaevitz, Laura and Garg, Neha and Balta, Ana-Maria and Rego, Meghan A and Gregory, Meredith and Jakobs, Tatjana C and Zhong, Lei and Wakimoto, Hiroko and Mostoslavsky, Raul and Tsubota, Kazuo and Bonasera, Stephen J and Palmeira, Carlos M and Seidman, Christine E and Wolf, Norman S and Kreiling, Jill A and Sedivy, John M and Murphy, George F and Green, Richard E and Garcia, Benjamin A and Berger, Shelley L and Shankland, Stuart J and Gladyshev, Vadim N and Ksander, Bruce R and Rajman, Luis A and Sinclair, David A},
	langid = {english},
	keywords = {❓ Multiple {DOI}},
	file = {Yang et al. - 1 Loss of epigenetic information as a cause of mam.pdf:/home/pars/Coding/papers/Zotero/storage/I88YFKVA/Yang et al. - 1 Loss of epigenetic information as a cause of mam.pdf:application/pdf},
}

@article{radford_language_2019,
	title = {Language Models are Unsupervised Multitask Learners},
	abstract = {Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspeciﬁc datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called {WebText}. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the {CoQA} dataset - matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, {GPT}-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underﬁts {WebText}. Samples from the model reﬂect these improvements and contain coherent paragraphs of text. These ﬁndings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.},
	author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
	date = {2019},
	langid = {english},
	keywords = {⛔ No {DOI} found},
	file = {Radford et al. - Language Models are Unsupervised Multitask Learner.pdf:/home/pars/Coding/papers/Zotero/storage/YRVVPXCS/Radford et al. - Language Models are Unsupervised Multitask Learner.pdf:application/pdf},
}

@misc{yang_diffusion_2022,
	title = {Diffusion Models: A Comprehensive Survey of Methods and Applications},
	url = {http://arxiv.org/abs/2209.00796},
	doi = {10.48550/arXiv.2209.00796},
	shorttitle = {Diffusion Models},
	abstract = {Diffusion models have emerged as a powerful new family of deep generative models with record-breaking performance in many applications, including image synthesis, video generation, and molecule design. In this survey, we provide an overview of the rapidly expanding body of work on diffusion models, categorizing the research into three key areas: efficient sampling, improved likelihood estimation, and handling data with special structures. We also discuss the potential for combining diffusion models with other generative models for enhanced results. We further review the wide-ranging applications of diffusion models in fields spanning from computer vision, natural language processing, temporal data modeling, to interdisciplinary applications in other scientific disciplines. This survey aims to provide a contextualized, in-depth look at the state of diffusion models, identifying the key areas of focus and pointing to potential areas for further exploration. Github: https://github.com/{YangLing}0818/Diffusion-Models-Papers-Survey-Taxonomy.},
	number = {{arXiv}:2209.00796},
	publisher = {{arXiv}},
	author = {Yang, Ling and Zhang, Zhilong and Song, Yang and Hong, Shenda and Xu, Runsheng and Zhao, Yue and Shao, Yingxia and Zhang, Wentao and Cui, Bin and Yang, Ming-Hsuan},
	urldate = {2023-01-25},
	date = {2022-10-23},
	eprinttype = {arxiv},
	eprint = {2209.00796 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	annotation = {Comment: 39 pages, 9 figures, citing 289 papers, project: https://github.com/{YangLing}0818/Diffusion-Models-Papers-Survey-Taxonomy},
	file = {arXiv Fulltext PDF:/home/pars/Coding/papers/Zotero/storage/RZXBJXTR/Yang et al. - 2022 - Diffusion Models A Comprehensive Survey of Method.pdf:application/pdf;arXiv.org Snapshot:/home/pars/Coding/papers/Zotero/storage/R7GQQ2PX/Yang et al. - 2022 - Diffusion Models A Comprehensive Survey of Method.html:text/html},
}

@misc{mildenhall_nerf_2020,
	title = {{NeRF}: Representing Scenes as Neural Radiance Fields for View Synthesis},
	url = {http://arxiv.org/abs/2003.08934},
	doi = {10.48550/arXiv.2003.08934},
	shorttitle = {{NeRF}},
	abstract = {We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location \$(x,y,z)\$ and viewing direction \$({\textbackslash}theta, {\textbackslash}phi)\$) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.},
	number = {{arXiv}:2003.08934},
	publisher = {{arXiv}},
	author = {Mildenhall, Ben and Srinivasan, Pratul P. and Tancik, Matthew and Barron, Jonathan T. and Ramamoorthi, Ravi and Ng, Ren},
	urldate = {2023-01-26},
	date = {2020-08-03},
	eprinttype = {arxiv},
	eprint = {2003.08934 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
	annotation = {Comment: {ECCV} 2020 (oral). Project page with videos and code: http://tancik.com/nerf},
	file = {arXiv Fulltext PDF:/home/pars/Coding/papers/Zotero/storage/B7A2I58V/Mildenhall et al. - 2020 - NeRF Representing Scenes as Neural Radiance Field.pdf:application/pdf;arXiv.org Snapshot:/home/pars/Coding/papers/Zotero/storage/Z9PLX9DF/2003.html:text/html},
}

@misc{zhang_nerf_2020,
	title = {{NeRF}++: Analyzing and Improving Neural Radiance Fields},
	url = {http://arxiv.org/abs/2010.07492},
	shorttitle = {{NeRF}++},
	abstract = {Neural Radiance Fields ({NeRF}) achieve impressive view synthesis results for a variety of capture settings, including 360 capture of bounded scenes and forward-facing capture of bounded and unbounded scenes. {NeRF} fits multi-layer perceptrons ({MLPs}) representing view-invariant opacity and view-dependent color volumes to a set of training images, and samples novel views based on volume rendering techniques. In this technical report, we first remark on radiance fields and their potential ambiguities, namely the shape-radiance ambiguity, and analyze {NeRF}'s success in avoiding such ambiguities. Second, we address a parametrization issue involved in applying {NeRF} to 360 captures of objects within large-scale, unbounded 3D scenes. Our method improves view synthesis fidelity in this challenging scenario. Code is available at https://github.com/Kai-46/nerfplusplus.},
	number = {{arXiv}:2010.07492},
	publisher = {{arXiv}},
	author = {Zhang, Kai and Riegler, Gernot and Snavely, Noah and Koltun, Vladlen},
	urldate = {2023-01-26},
	date = {2020-10-21},
	eprinttype = {arxiv},
	eprint = {2010.07492 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annotation = {Comment: Code is available at https://github.com/Kai-46/nerfplusplus; fix a minor formatting issue in Fig. 4},
	file = {arXiv Fulltext PDF:/home/pars/Coding/papers/Zotero/storage/FZLKQ64E/Zhang et al. - 2020 - NeRF++ Analyzing and Improving Neural Radiance Fi.pdf:application/pdf;arXiv.org Snapshot:/home/pars/Coding/papers/Zotero/storage/4BQWU6KQ/2010.html:text/html},
}

@article{du_vos_2022,
	title = {Vos: Learning what you don't know by virtual outlier synthesis},
	shorttitle = {Vos},
	journaltitle = {{arXiv} preprint {arXiv}:2202.01197},
	author = {Du, Xuefeng and Wang, Zhaoning and Cai, Mu and Li, Yixuan},
	date = {2022},
	keywords = {⛔ No {DOI} found},
	file = {Full Text:/home/pars/Coding/papers/Zotero/storage/PQEFYMAN/Du et al. - 2022 - Vos Learning what you don't know by virtual outli.pdf:application/pdf;Snapshot:/home/pars/Coding/papers/Zotero/storage/YQ8YQLPH/2202.html:text/html},
}

@misc{ho_denoising_2020,
	title = {Denoising Diffusion Probabilistic Models},
	url = {http://arxiv.org/abs/2006.11239},
	doi = {10.48550/arXiv.2006.11239},
	abstract = {We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional {CIFAR}10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art {FID} score of 3.17. On 256x256 {LSUN}, we obtain sample quality similar to {ProgressiveGAN}. Our implementation is available at https://github.com/hojonathanho/diffusion},
	number = {{arXiv}:2006.11239},
	publisher = {{arXiv}},
	author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
	urldate = {2023-01-28},
	date = {2020-12-16},
	eprinttype = {arxiv},
	eprint = {2006.11239 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/pars/Coding/papers/Zotero/storage/AL63AUMR/Ho et al. - 2020 - Denoising Diffusion Probabilistic Models.pdf:application/pdf;arXiv.org Snapshot:/home/pars/Coding/papers/Zotero/storage/4PQ4KJCR/2006.html:text/html},
}

@misc{norcliffe_neural_2021,
	title = {Neural {ODE} Processes},
	url = {http://arxiv.org/abs/2103.12413},
	abstract = {Neural Ordinary Differential Equations ({NODEs}) use a neural network to model the instantaneous rate of change in the state of a system. However, despite their apparent suitability for dynamics-governed time-series, {NODEs} present a few disadvantages. First, they are unable to adapt to incoming data points, a fundamental requirement for real-time applications imposed by the natural direction of time. Second, time series are often composed of a sparse set of measurements that could be explained by many possible underlying dynamics. {NODEs} do not capture this uncertainty. In contrast, Neural Processes ({NPs}) are a family of models providing uncertainty estimation and fast data adaptation but lack an explicit treatment of the flow of time. To address these problems, we introduce Neural {ODE} Processes ({NDPs}), a new class of stochastic processes determined by a distribution over Neural {ODEs}. By maintaining an adaptive data-dependent distribution over the underlying {ODE}, we show that our model can successfully capture the dynamics of low-dimensional systems from just a few data points. At the same time, we demonstrate that {NDPs} scale up to challenging high-dimensional time-series with unknown latent dynamics such as rotating {MNIST} digits.},
	number = {{arXiv}:2103.12413},
	publisher = {{arXiv}},
	author = {Norcliffe, Alexander and Bodnar, Cristian and Day, Ben and Moss, Jacob and Liò, Pietro},
	urldate = {2023-01-28},
	date = {2021-08-17},
	eprinttype = {arxiv},
	eprint = {2103.12413 [cs]},
	keywords = {Computer Science - Machine Learning},
	annotation = {Comment: {ICLR} 2021. 9 pages, 6 figures, 7 pages of appendices},
	file = {arXiv Fulltext PDF:/home/pars/Coding/papers/Zotero/storage/Y24YN7UM/Norcliffe et al. - 2021 - Neural ODE Processes.pdf:application/pdf;arXiv.org Snapshot:/home/pars/Coding/papers/Zotero/storage/F8CCVLWR/2103.html:text/html},
}

@misc{hanebeck_deterministic_2019,
	title = {Deterministic Sampling of Multivariate Densities based on Projected Cumulative Distributions},
	url = {http://arxiv.org/abs/1912.12875},
	abstract = {We want to approximate general multivariate probability density functions by deterministic sample sets. For optimal sampling, the closeness to the given continuous density has to be assessed. This is a difficult challenge in multivariate settings. Simple solutions are restricted to the one-dimensional case. In this paper, we propose to employ one-dimensional density projections. These are the Radon transforms of the densities. For every projection, we compute their cumulative distribution function. These Projected Cumulative Distributions ({PCDs}) are compared for all possible projections (or a discrete set thereof). This leads to a tractable distance measure in multivariate space. The proposed approximation method is efficient as calculating the distance measure mainly entails sorting in one dimension. It is also surprisingly simple to implement.},
	number = {{arXiv}:1912.12875},
	publisher = {{arXiv}},
	author = {Hanebeck, Uwe D.},
	urldate = {2023-01-27},
	date = {2019-12-30},
	eprinttype = {arxiv},
	eprint = {1912.12875 [cs, eess]},
	keywords = {Electrical Engineering and Systems Science - Systems and Control},
	annotation = {Comment: 21 pages, 10 figures},
	file = {arXiv Fulltext PDF:/home/pars/Coding/papers/Zotero/storage/GJKZI6F5/Hanebeck - 2019 - Deterministic Sampling of Multivariate Densities b.pdf:application/pdf;arXiv.org Snapshot:/home/pars/Coding/papers/Zotero/storage/AQ4DXTR4/1912.html:text/html},
}

@article{huber_gaussian_2008,
	title = {Gaussian Filter based on Deterministic Sampling for High Quality Nonlinear Estimation},
	volume = {41},
	issn = {14746670},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1474667016411572},
	doi = {10.3182/20080706-5-KR-1001.02291},
	abstract = {In this paper, a Gaussian ﬁlter for nonlinear Bayesian estimation is introduced that is based on a deterministic sample selection scheme. For an eﬀective sample selection, a parametric density function representation of the sample points is employed, which allows approximating the cumulative distribution function of the prior Gaussian density. The computationally demanding parts of the optimization problem formulated for approximation are carried out oﬀ-line for obtaining an eﬃcient ﬁlter, whose estimation quality can be altered by adjusting the number of used sample points. The improved performance of the proposed Gaussian ﬁlter compared to the well-known unscented Kalman ﬁlter is demonstrated by means of two examples.},
	pages = {13527--13532},
	number = {2},
	journaltitle = {{IFAC} Proceedings Volumes},
	shortjournal = {{IFAC} Proceedings Volumes},
	author = {Huber, Marco F. and Hanebeck, Uwe D.},
	urldate = {2023-01-27},
	date = {2008},
	langid = {english},
	file = {Huber and Hanebeck - 2008 - Gaussian Filter based on Deterministic Sampling fo.pdf:/home/pars/Coding/papers/Zotero/storage/ITFX7X5G/Huber and Hanebeck - 2008 - Gaussian Filter based on Deterministic Sampling fo.pdf:application/pdf},
}

@article{brown_language_2020,
	title = {Language models are few-shot learners},
	volume = {33},
	pages = {1877--1901},
	journaltitle = {Advances in neural information processing systems},
	author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D. and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda},
	date = {2020},
	keywords = {⛔ No {DOI} found},
	file = {Full Text:/home/pars/Coding/papers/Zotero/storage/5W6NQ4QX/Brown et al. - 2020 - Language models are few-shot learners.pdf:application/pdf;Snapshot:/home/pars/Coding/papers/Zotero/storage/LLLN873E/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html:text/html},
}

@article{luo_mof_2022,
	title = {{MOF} Synthesis Prediction Enabled by Automatic Data Mining and Machine Learning**},
	volume = {61},
	issn = {1521-3773},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/anie.202200242},
	doi = {10.1002/anie.202200242},
	abstract = {Despite rapid progress in the field of metal–organic frameworks ({MOFs}), the potential of using machine learning ({ML}) methods to predict {MOF} synthesis parameters is still untapped. Here, we show how {ML} can be used for rationalization and acceleration of the {MOF} discovery process by directly predicting the synthesis conditions of a {MOF} based on its crystal structure. Our approach is based on: i) establishing the first {MOF} synthesis database via automatic extraction of synthesis parameters from the literature, ii) training and optimizing {ML} models by employing the {MOF} database, and iii) predicting the synthesis conditions for new {MOF} structures. The {ML} models, even at an initial stage, exhibit a good prediction performance, outperforming human expert predictions, obtained through a synthesis survey. The automated synthesis prediction is available via a web-tool on https://mof-synthesis.aimat.science.},
	pages = {e202200242},
	number = {19},
	journaltitle = {Angewandte Chemie International Edition},
	author = {Luo, Yi and Bag, Saientan and Zaremba, Orysia and Cierpka, Adrian and Andreo, Jacopo and Wuttke, Stefan and Friederich, Pascal and Tsotsalas, Manuel},
	urldate = {2023-02-01},
	date = {2022},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/anie.202200242},
	keywords = {Data Mining, Machine Learning, Metal–Organic Frameworks, Microporous Materials, Synthesis Prediction},
	file = {Full Text PDF:/home/pars/Coding/papers/Zotero/storage/A2FX6FQ3/Luo et al. - 2022 - MOF Synthesis Prediction Enabled by Automatic Data.pdf:application/pdf;Snapshot:/home/pars/Coding/papers/Zotero/storage/NE6EQI5Y/anie.html:text/html},
}

@article{hawizy_chemicaltagger_2011,
	title = {{ChemicalTagger}: A tool for semantic text-mining in chemistry},
	volume = {3},
	issn = {1758-2946},
	url = {https://doi.org/10.1186/1758-2946-3-17},
	doi = {10.1186/1758-2946-3-17},
	shorttitle = {{ChemicalTagger}},
	abstract = {The primary method for scientific communication is in the form of published scientific articles and theses which use natural language combined with domain-specific terminology. As such, they contain free owing unstructured text. Given the usefulness of data extraction from unstructured literature, we aim to show how this can be achieved for the discipline of chemistry. The highly formulaic style of writing most chemists adopt make their contributions well suited to high-throughput Natural Language Processing ({NLP}) approaches.},
	pages = {17},
	number = {1},
	journaltitle = {Journal of Cheminformatics},
	shortjournal = {Journal of Cheminformatics},
	author = {Hawizy, Lezan and Jessop, David M. and Adams, Nico and Murray-Rust, Peter},
	urldate = {2023-02-01},
	date = {2011-05-16},
	keywords = {Atom Transfer Radical Polymerization, Chemical Entity, Dice Coefficient, Natural Language Processing, Potassium Carbonate},
	file = {Full Text PDF:/home/pars/Coding/papers/Zotero/storage/X4GDI2LY/Hawizy et al. - 2011 - ChemicalTagger A tool for semantic text-mining in.pdf:application/pdf;Snapshot:/home/pars/Coding/papers/Zotero/storage/ZJSDQZ6L/1758-2946-3-17.html:text/html},
}

@misc{dunn_structured_2022,
	title = {Structured information extraction from complex scientific text with fine-tuned large language models},
	url = {http://arxiv.org/abs/2212.05238},
	doi = {10.48550/arXiv.2212.05238},
	abstract = {Intelligently extracting and linking complex scientific information from unstructured text is a challenging endeavor particularly for those inexperienced with natural language processing. Here, we present a simple sequence-to-sequence approach to joint named entity recognition and relation extraction for complex hierarchical information in scientific text. The approach leverages a pre-trained large language model ({LLM}), {GPT}-3, that is fine-tuned on approximately 500 pairs of prompts (inputs) and completions (outputs). Information is extracted either from single sentences or across sentences in abstracts/passages, and the output can be returned as simple English sentences or a more structured format, such as a list of {JSON} objects. We demonstrate that {LLMs} trained in this way are capable of accurately extracting useful records of complex scientific knowledge for three representative tasks in materials chemistry: linking dopants with their host materials, cataloging metal-organic frameworks, and general chemistry/phase/morphology/application information extraction. This approach represents a simple, accessible, and highly-flexible route to obtaining large databases of structured knowledge extracted from unstructured text. An online demo is available at http://www.matscholar.com/info-extraction.},
	number = {{arXiv}:2212.05238},
	publisher = {{arXiv}},
	author = {Dunn, Alexander and Dagdelen, John and Walker, Nicholas and Lee, Sanghoon and Rosen, Andrew S. and Ceder, Gerbrand and Persson, Kristin and Jain, Anubhav},
	urldate = {2023-02-01},
	date = {2022-12-10},
	eprinttype = {arxiv},
	eprint = {2212.05238 [cond-mat]},
	keywords = {Computer Science - Computation and Language, Condensed Matter - Materials Science, I.7.m},
	file = {arXiv Fulltext PDF:/home/pars/Coding/papers/Zotero/storage/SAXMQRUZ/Dunn et al. - 2022 - Structured information extraction from complex sci.pdf:application/pdf;arXiv.org Snapshot:/home/pars/Coding/papers/Zotero/storage/2GMEJZJQ/2212.html:text/html},
}

@misc{zhang_opt_2022,
	title = {{OPT}: Open Pre-trained Transformer Language Models},
	url = {http://arxiv.org/abs/2205.01068},
	shorttitle = {{OPT}},
	abstract = {Large language models, which are often trained for hundreds of thousands of compute days, have shown remarkable capabilities for zero- and few-shot learning. Given their computational cost, these models are difficult to replicate without significant capital. For the few that are available through {APIs}, no access is granted to the full model weights, making them difficult to study. We present Open Pre-trained Transformers ({OPT}), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers. We show that {OPT}-175B is comparable to {GPT}-3, while requiring only 1/7th the carbon footprint to develop. We are also releasing our logbook detailing the infrastructure challenges we faced, along with code for experimenting with all of the released models.},
	number = {{arXiv}:2205.01068},
	publisher = {{arXiv}},
	author = {Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and Mihaylov, Todor and Ott, Myle and Shleifer, Sam and Shuster, Kurt and Simig, Daniel and Koura, Punit Singh and Sridhar, Anjali and Wang, Tianlu and Zettlemoyer, Luke},
	urldate = {2023-02-01},
	date = {2022-06-21},
	eprinttype = {arxiv},
	eprint = {2205.01068 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/pars/Coding/papers/Zotero/storage/D9A7UJIS/Zhang et al. - 2022 - OPT Open Pre-trained Transformer Language Models.pdf:application/pdf;arXiv.org Snapshot:/home/pars/Coding/papers/Zotero/storage/AFU7JYUG/2205.html:text/html},
}

@inproceedings{dong_towards_2020,
	title = {Towards Adaptive Residual Network Training: A Neural-{ODE} Perspective},
	url = {https://proceedings.mlr.press/v119/dong20c.html},
	shorttitle = {Towards Adaptive Residual Network Training},
	abstract = {In pursuit of resource-economical machine learning, attempts have been made to dynamically adjust computation workloads in different training stages, i.e., starting with a shallow network and gradually increasing the model depth (and computation workloads) during training. However, there is neither guarantee nor guidance on designing such network grow, due to the lack of its theoretical underpinnings. In this work, to explore the theory behind, we conduct theoretical analyses from an ordinary differential equation perspective. Specifically, we illustrate the dynamics of network growth and propose a novel performance measure specific to the depth increase. Illuminated by our analyses, we move towards theoretically sound growing operations and schedulers, giving rise to an adaptive training algorithm for residual networks, {LipGrow}, which automatically increases network depth thus accelerates training. In our experiments, it achieves comparable performance while reducing ∼ 50\% of training time.},
	eventtitle = {International Conference on Machine Learning},
	pages = {2616--2626},
	booktitle = {Proceedings of the 37th International Conference on Machine Learning},
	publisher = {{PMLR}},
	author = {Dong, Chengyu and Liu, Liyuan and Li, Zichao and Shang, Jingbo},
	urldate = {2023-02-04},
	date = {2020-11-21},
	langid = {english},
	file = {Full Text PDF:/home/pars/Coding/papers/Zotero/storage/QKZEZP2E/Dong et al. - 2020 - Towards Adaptive Residual Network Training A Neur.pdf:application/pdf;Supplementary PDF:/home/pars/Coding/papers/Zotero/storage/R66QVR5B/Dong et al. - 2020 - Towards Adaptive Residual Network Training A Neur.pdf:application/pdf},
}

@inproceedings{ramesh_zero-shot_2021,
	title = {Zero-Shot Text-to-Image Generation},
	url = {https://proceedings.mlr.press/v139/ramesh21a.html},
	abstract = {Text-to-image generation has traditionally focused on finding better modeling assumptions for training on a fixed dataset. These assumptions might involve complex architectures, auxiliary losses, or side information such as object part labels or segmentation masks supplied during training. We describe a simple approach for this task based on a transformer that autoregressively models the text and image tokens as a single stream of data. With sufficient data and scale, our approach is competitive with previous domain-specific models when evaluated in a zero-shot fashion.},
	eventtitle = {International Conference on Machine Learning},
	pages = {8821--8831},
	booktitle = {Proceedings of the 38th International Conference on Machine Learning},
	publisher = {{PMLR}},
	author = {Ramesh, Aditya and Pavlov, Mikhail and Goh, Gabriel and Gray, Scott and Voss, Chelsea and Radford, Alec and Chen, Mark and Sutskever, Ilya},
	urldate = {2023-02-04},
	date = {2021-07-01},
	langid = {english},
	note = {{ISSN}: 2640-3498},
	file = {Full Text PDF:/home/pars/Coding/papers/Zotero/storage/7C5K3MWF/Ramesh et al. - 2021 - Zero-Shot Text-to-Image Generation.pdf:application/pdf},
}

@misc{hoffmann_training_2022,
	title = {Training Compute-Optimal Large Language Models},
	url = {http://arxiv.org/abs/2203.15556},
	doi = {10.48550/arXiv.2203.15556},
	abstract = {We investigate the optimal model size and number of tokens for training a transformer language model under a given compute budget. We find that current large language models are significantly undertrained, a consequence of the recent focus on scaling language models whilst keeping the amount of training data constant. By training over 400 language models ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens, we find that for compute-optimal training, the model size and the number of training tokens should be scaled equally: for every doubling of model size the number of training tokens should also be doubled. We test this hypothesis by training a predicted compute-optimal model, Chinchilla, that uses the same compute budget as Gopher but with 70B parameters and 4\${\textbackslash}times\$ more more data. Chinchilla uniformly and significantly outperforms Gopher (280B), {GPT}-3 (175B), Jurassic-1 (178B), and Megatron-Turing {NLG} (530B) on a large range of downstream evaluation tasks. This also means that Chinchilla uses substantially less compute for fine-tuning and inference, greatly facilitating downstream usage. As a highlight, Chinchilla reaches a state-of-the-art average accuracy of 67.5\% on the {MMLU} benchmark, greater than a 7\% improvement over Gopher.},
	number = {{arXiv}:2203.15556},
	publisher = {{arXiv}},
	author = {Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and Hennigan, Tom and Noland, Eric and Millican, Katie and Driessche, George van den and Damoc, Bogdan and Guy, Aurelia and Osindero, Simon and Simonyan, Karen and Elsen, Erich and Rae, Jack W. and Vinyals, Oriol and Sifre, Laurent},
	urldate = {2023-02-06},
	date = {2022-03-29},
	eprinttype = {arxiv},
	eprint = {2203.15556 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/pars/Coding/papers/Zotero/storage/FCZT9E26/Hoffmann et al. - 2022 - Training Compute-Optimal Large Language Models.pdf:application/pdf;arXiv.org Snapshot:/home/pars/Coding/papers/Zotero/storage/9TNRDKT7/2203.html:text/html},
}

@misc{borji_categorical_2023,
	title = {A Categorical Archive of {ChatGPT} Failures},
	url = {http://arxiv.org/abs/2302.03494},
	doi = {10.48550/arXiv.2302.03494},
	abstract = {Large language models have been demonstrated to be valuable in different fields. {ChatGPT}, developed by {OpenAI}, has been trained using massive amounts of data and simulates human conversation by comprehending context and generating appropriate responses. It has garnered significant attention due to its ability to effectively answer a broad range of human inquiries, with fluent and comprehensive answers surpassing prior public chatbots in both security and usefulness. However, a comprehensive analysis of {ChatGPT}'s failures is lacking, which is the focus of this study. Ten categories of failures, including reasoning, factual errors, math, coding, and bias, are presented and discussed. The risks, limitations, and societal implications of {ChatGPT} are also highlighted. The goal of this study is to assist researchers and developers in enhancing future language models and chatbots.},
	number = {{arXiv}:2302.03494},
	publisher = {{arXiv}},
	author = {Borji, Ali},
	urldate = {2023-02-08},
	date = {2023-02-05},
	eprinttype = {arxiv},
	eprint = {2302.03494 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/home/pars/Coding/papers/Zotero/storage/TJ4SEJ55/Borji - 2023 - A Categorical Archive of ChatGPT Failures.pdf:application/pdf;arXiv.org Snapshot:/home/pars/Coding/papers/Zotero/storage/942FKWH4/2302.html:text/html},
}

@misc{creswell_selection-inference_2022,
	title = {Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning},
	url = {http://arxiv.org/abs/2205.09712},
	doi = {10.48550/arXiv.2205.09712},
	shorttitle = {Selection-Inference},
	abstract = {Large language models ({LLMs}) have been shown to be capable of impressive few-shot generalisation to new tasks. However, they still tend to perform poorly on multi-step logical reasoning problems. Here we carry out a comprehensive evaluation of {LLMs} on 50 tasks that probe different aspects of logical reasoning. We show that language models tend to perform fairly well at single step inference or entailment tasks, but struggle to chain together multiple reasoning steps to solve more complex problems. In light of this, we propose a Selection-Inference ({SI}) framework that exploits pre-trained {LLMs} as general processing modules, and alternates between selection and inference to generate a series of interpretable, casual reasoning steps leading to the final answer. We show that a 7B parameter {LLM} used within the {SI} framework in a 5-shot generalisation setting, with no fine-tuning, yields a performance improvement of over 100\% compared to an equivalent vanilla baseline on a suite of 10 logical reasoning tasks. The same model in the same setting even outperforms a significantly larger 280B parameter baseline on the same suite of tasks. Moreover, answers produced by the {SI} framework are accompanied by a causal natural-language-based reasoning trace, which has important implications for the safety and trustworthiness of the system.},
	number = {{arXiv}:2205.09712},
	publisher = {{arXiv}},
	author = {Creswell, Antonia and Shanahan, Murray and Higgins, Irina},
	urldate = {2023-02-09},
	date = {2022-05-19},
	eprinttype = {arxiv},
	eprint = {2205.09712 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/home/pars/Coding/papers/Zotero/storage/L8F4U82R/Creswell et al. - 2022 - Selection-Inference Exploiting Large Language Mod.pdf:application/pdf;arXiv.org Snapshot:/home/pars/Coding/papers/Zotero/storage/4IZSSLEP/2205.html:text/html},
}

@book{roberts_principles_2022,
	title = {The Principles of Deep Learning Theory},
	url = {http://arxiv.org/abs/2106.10165},
	abstract = {This book develops an effective theory approach to understanding deep neural networks of practical relevance. Beginning from a first-principles component-level picture of networks, we explain how to determine an accurate description of the output of trained networks by solving layer-to-layer iteration equations and nonlinear learning dynamics. A main result is that the predictions of networks are described by nearly-Gaussian distributions, with the depth-to-width aspect ratio of the network controlling the deviations from the infinite-width Gaussian description. We explain how these effectively-deep networks learn nontrivial representations from training and more broadly analyze the mechanism of representation learning for nonlinear models. From a nearly-kernel-methods perspective, we find that the dependence of such models' predictions on the underlying learning algorithm can be expressed in a simple and universal way. To obtain these results, we develop the notion of representation group flow ({RG} flow) to characterize the propagation of signals through the network. By tuning networks to criticality, we give a practical solution to the exploding and vanishing gradient problem. We further explain how {RG} flow leads to near-universal behavior and lets us categorize networks built from different activation functions into universality classes. Altogether, we show that the depth-to-width ratio governs the effective model complexity of the ensemble of trained networks. By using information-theoretic techniques, we estimate the optimal aspect ratio at which we expect the network to be practically most useful and show how residual connections can be used to push this scale to arbitrary depths. With these tools, we can learn in detail about the inductive bias of architectures, hyperparameters, and optimizers.},
	author = {Roberts, Daniel A. and Yaida, Sho and Hanin, Boris},
	urldate = {2023-02-09},
	date = {2022-05-26},
	doi = {10.1017/9781009023405},
	eprinttype = {arxiv},
	eprint = {2106.10165 [hep-th, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence, High Energy Physics - Theory},
	annotation = {Comment: 471 pages, to be published by Cambridge University Press; v2: hyperlinks fixed, index added},
	file = {arXiv Fulltext PDF:/home/pars/Coding/papers/Zotero/storage/GU8KC7M4/Roberts et al. - 2022 - The Principles of Deep Learning Theory.pdf:application/pdf;arXiv.org Snapshot:/home/pars/Coding/papers/Zotero/storage/X9VLQDY2/2106.html:text/html},
}

@misc{schick_toolformer_2023,
	title = {Toolformer: Language Models Can Teach Themselves to Use Tools},
	url = {http://arxiv.org/abs/2302.04761},
	doi = {10.48550/arXiv.2302.04761},
	shorttitle = {Toolformer},
	abstract = {Language models ({LMs}) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that {LMs} can teach themselves to use external tools via simple {APIs} and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which {APIs} to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each {API}. We incorporate a range of tools, including a calculator, a Q{\textbackslash}\&A system, two different search engines, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities.},
	number = {{arXiv}:2302.04761},
	publisher = {{arXiv}},
	author = {Schick, Timo and Dwivedi-Yu, Jane and Dessì, Roberto and Raileanu, Roberta and Lomeli, Maria and Zettlemoyer, Luke and Cancedda, Nicola and Scialom, Thomas},
	urldate = {2023-02-11},
	date = {2023-02-09},
	eprinttype = {arxiv},
	eprint = {2302.04761 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/home/pars/Coding/papers/Zotero/storage/JQ2VH8LX/Schick et al. - 2023 - Toolformer Language Models Can Teach Themselves t.pdf:application/pdf;arXiv.org Snapshot:/home/pars/Coding/papers/Zotero/storage/5IMA8GLU/2302.html:text/html},
}
